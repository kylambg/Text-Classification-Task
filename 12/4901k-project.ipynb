{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 12  \n",
    "Authors: Lam Kwun Yuk, Sha Yu Hin  \n",
    "IDs: 20512073, 20516835  \n",
    "\n",
    "Most code used are from tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/4901k-project/val.pkl\n",
      "/kaggle/input/4901k-project/test.pkl\n",
      "/kaggle/input/4901k-project/train.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy.random import seed\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, Average,Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "test_dict = pkl.load(open('/kaggle/input/4901k-project/test.pkl', \"rb\"))\n",
    "train_dict = pkl.load(open('/kaggle/input/4901k-project/train.pkl', \"rb\"))\n",
    "val_dict = pkl.load(open('/kaggle/input/4901k-project/val.pkl', \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protection', 'of', 'calves', 'against', 'fatal', 'enteric', 'colibacillosis', 'by', 'orally', 'administered', 'Escherichia', 'coli', 'K99', '-', 'specific', 'monoclonal', 'antibody', '.', 'A', 'monoclonal', 'antibody', '(', 'MCA', ')', 'to', 'enterotoxigenic', 'Escherichia', 'coli', 'K99', 'antigen', 'agglutinated', 'K99+', 'enterotoxigenic', 'E', '.', 'coli', 'strains', 'B44', '(', 'O9', ':', 'K30', ';', 'K99', ';', 'F41', ':', 'H-', ')', 'and', 'B41', '(', 'O101', ':', 'K99', ';', 'F41', ':', 'H-', ')', 'grown', 'at', '37', 'degrees', 'C', 'but', 'not', 'at', '18', 'degrees', 'C.', 'The', 'MCA', ',', 'which', 'was', 'characterized', 'as', 'immunoglobulin', 'G1', ',', 'reacted', 'specifically', 'with', 'K99', 'antigen', 'in', 'an', 'enzyme-linked', 'immunosorbent', 'assay', 'and', 'precipitated', 'radiolabeled', 'K99', 'antigen', '.', 'A', 'total', 'of', '45', 'colostrum', '-fed', 'and', 'colostrum', '-deprived', 'calves', 'were', 'used', 'in', 'three', 'separate', 'trials', 'to', 'determine', 'whether', 'the', 'orally', 'administered', 'K99-specific', 'MCA', 'would', 'prevent', 'diarrhea', 'caused', 'by', 'strain', 'B44']\n",
      "['O', 'O', 'LIVESTOCK', 'O', 'O', 'DISEASE_OR_SYNDROME', 'DISEASE_OR_SYNDROME', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'GENE_OR_GENOME', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'O', 'LIVESTOCK', 'O', 'O', 'O', 'CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'DISEASE_OR_SYNDROME', 'O', 'O', 'O', 'GENE_OR_GENOME']\n",
      "index: 0\n",
      "('Protection', 'O') ('of', 'O') ('calves', 'LIVESTOCK') ('against', 'O') ('fatal', 'O') ('enteric', 'DISEASE_OR_SYNDROME') ('colibacillosis', 'DISEASE_OR_SYNDROME') ('by', 'O') ('orally', 'GENE_OR_GENOME') ('administered', 'GENE_OR_GENOME') ('Escherichia', 'GENE_OR_GENOME') ('coli', 'GENE_OR_GENOME') ('K99', 'GENE_OR_GENOME') ('-', 'O') ('specific', 'CARDINAL') ('monoclonal', 'CARDINAL') ('antibody', 'CARDINAL') ('.', 'O') ('A', 'O') ('monoclonal', 'CHEMICAL') ('antibody', 'CHEMICAL') ('(', 'O') ('MCA', 'GENE_OR_GENOME') (')', 'O') ('to', 'O') ('enterotoxigenic', 'CHEMICAL') ('Escherichia', 'CHEMICAL') ('coli', 'CHEMICAL') ('K99', 'O') ('antigen', 'O') ('agglutinated', 'O') ('K99+', 'GENE_OR_GENOME') ('enterotoxigenic', 'GENE_OR_GENOME') ('E', 'GENE_OR_GENOME') ('.', 'O') ('coli', 'CHEMICAL') ('strains', 'CHEMICAL') ('B44', 'CHEMICAL') ('(', 'O') ('O9', 'O') (':', 'O') ('K30', 'O') (';', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('and', 'O') ('B41', 'CHEMICAL') ('(', 'O') ('O101', 'PRODUCT') (':', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('grown', 'O') ('at', 'O') ('37', 'QUANTITY') ('degrees', 'QUANTITY') ('C', 'O') ('but', 'O') ('not', 'O') ('at', 'O') ('18', 'QUANTITY') ('degrees', 'QUANTITY') ('C.', 'O') ('The', 'O') ('MCA', 'GENE_OR_GENOME') (',', 'O') ('which', 'O') ('was', 'O') ('characterized', 'O') ('as', 'O') ('immunoglobulin', 'GENE_OR_GENOME') ('G1', 'GENE_OR_GENOME') (',', 'O') ('reacted', 'O') ('specifically', 'O') ('with', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('in', 'O') ('an', 'O') ('enzyme-linked', 'CHEMICAL') ('immunosorbent', 'CHEMICAL') ('assay', 'CHEMICAL') ('and', 'O') ('precipitated', 'O') ('radiolabeled', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('.', 'O') ('A', 'O') ('total', 'O') ('of', 'O') ('45', 'O') ('colostrum', 'CHEMICAL') ('-fed', 'O') ('and', 'O') ('colostrum', 'CHEMICAL') ('-deprived', 'O') ('calves', 'LIVESTOCK') ('were', 'O') ('used', 'O') ('in', 'O') ('three', 'CARDINAL') ('separate', 'O') ('trials', 'O') ('to', 'O') ('determine', 'O') ('whether', 'O') ('the', 'O') ('orally', 'O') ('administered', 'O') ('K99-specific', 'O') ('MCA', 'GENE_OR_GENOME') ('would', 'O') ('prevent', 'O') ('diarrhea', 'DISEASE_OR_SYNDROME') ('caused', 'O') ('by', 'O') ('strain', 'O') ('B44', 'GENE_OR_GENOME')\n"
     ]
    }
   ],
   "source": [
    "# an entry of the dataset\n",
    "print(train_dict[\"word_seq\"][0])\n",
    "print(train_dict[\"tag_seq\"][0])\n",
    "print(\"index:\", train_dict[\"id\"][0])\n",
    "zipped = zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0])\n",
    "print(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of the NER tags: 65\n",
      "all the NER tags: {'PERSON', 'GROUP', 'EUKARYOTE', 'RESEARCH_ACTIVITY', 'IMMUNE_RESPONSE', 'MOLECULAR_FUNCTION', 'EXPERIMENTAL_MODEL_OF_DISEASE', 'EDUCATIONAL_ACTIVITY', 'VIRAL_PROTEIN', 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS', 'FAC', 'PRODUCT', 'WILDLIFE', 'LIVESTOCK', 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY', 'ORG', 'LABORATORY_OR_TEST_RESULT', 'TISSUE', 'GPE', 'ANATOMICAL_STRUCTURE', 'CARDINAL', 'MONEY', 'EVENT', 'DATE', 'BACTERIUM', 'ORGANISM', 'LOC', 'LAW', 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE', 'BODY_SUBSTANCE', 'ORGAN_OR_TISSUE_FUNCTION', 'QUANTITY', 'TIME', 'GENE_OR_GENOME', 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT', 'NORP', 'LANGUAGE', 'GROUP_ATTRIBUTE', 'O', 'PHYSICAL_SCIENCE', 'CELL_OR_MOLECULAR_DYSFUNCTION', 'CELL_FUNCTION', 'CHEMICAL', 'MACHINE_ACTIVITY', 'PERCENT', 'SOCIAL_BEHAVIOR', 'DISEASE_OR_SYNDROME', 'CORONAVIRUS', 'SUBSTRATE', 'MATERIAL', 'INJURY_OR_POISONING', 'DAILY_OR_RECREATIONAL_ACTIVITY', 'INDIVIDUAL_BEHAVIOR', 'WORK_OF_ART', 'EVOLUTION', 'CELL', 'ORDINAL', 'FOOD', 'VIRUS', 'ARCHAEON', 'DIAGNOSTIC_PROCEDURE', 'LABORATORY_PROCEDURE', '_t_pad_', 'CELL_COMPONENT', 'SIGN_OR_SYMPTOM'}\n"
     ]
    }
   ],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['word_seq'] = [stem(tokens) for tokens in train_dict[\"word_seq\"]]\n",
    "val_dict[\"word_seq\"] = [stem(tokens) for tokens in val_dict[\"word_seq\"]]\n",
    "test_dict[\"word_seq\"] = [stem(tokens) for tokens in test_dict[\"word_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word vocab: 55469 size of tag_dict: 65\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}            \n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (23600, 128) tag size: (23600, 128, 65)\n",
      "validating size: (2950, 128) tag size: (2950, 128, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              kernel_sizes, num_filters, num_mlp_layers,\n",
    "              padding=\"valid\",\n",
    "              strides=1,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param kernel_sizes: the kernel sizes of convolutional layers, type: list\n",
    "    :param num_filters: the number of filters for each kernel, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param padding: the padding method in convolutional layers, type: str\n",
    "    :param strides: the strides in convolutional layers, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a CNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # convolutional layers document: https://keras.io/layers/convolutional\n",
    "    # pooling layers document: https://keras.io/layers/pooling/\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    print(input_length,vocab_size,embedding_size,output_size)\n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ########### Conv-Pool ##########\n",
    "    ################################\n",
    "    # convolutional and pooling layers\n",
    "    cnn_results = list()\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # add convolutional layer\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=(kernel_size,),\n",
    "                      padding=padding,\n",
    "                      strides=strides)(emb)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            conv = BatchNormalization()(conv)\n",
    "        # add activation\n",
    "        conv = Activation(activation)(conv)\n",
    "        # add max-pooling\n",
    "        #maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)\n",
    "        #cnn_results.append(Flatten()(maxpool))\n",
    "        cnn_results.append(conv)\n",
    "    \n",
    "    ################################\n",
    "    ##### Fully Connected Layer ####\n",
    "    ################################\n",
    "    h = Average()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    #h = Concatenate()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    h = Dropout(dropout_rate, seed=0)(h)\n",
    "    #h = Embedding(input_dim=vocab_size,\n",
    "    #                output_dim=embedding_size,\n",
    "    #                input_length=input_length,\n",
    "    #                embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(h)\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add skip connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\",\n",
    "              return_sequences=True):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    # Referennce: https://keras.io/api/layers/#recurrent-layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            h = Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True))(h)\n",
    "            # return_sequences:\n",
    "            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "            # [h_1, h_2, ..., h_n] or h_n\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "\n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\",\n",
    "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "              bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)\n",
    "\n",
    "def evaluate(pred_file, ground_file):\n",
    "    file_dict = pkl.load(open(ground_file, \"rb\"))\n",
    "    file_preds = pd.read_csv(pred_file)\n",
    "    return calc_accuracy(np.array([json.loads(line) for line in file_preds[\"labels\"]]), \n",
    "              np.array(file_dict[\"tag_seq\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method seed of numpy.random.mtrand.RandomState object at 0x7fc0094ea9e0>\n",
      "128 55469 128 65\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 128, 128)          16512     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 128)          16512     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,141,441\n",
      "Trainable params: 7,141,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.2096 - accuracy: 0.7646\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82746, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 19s 91ms/step - loss: 1.2096 - accuracy: 0.7646 - val_loss: 0.6638 - val_accuracy: 0.8275\n",
      "Epoch 2/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.8432\n",
      "Epoch 00002: val_accuracy improved from 0.82746 to 0.85139, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 19s 87ms/step - loss: 0.5815 - accuracy: 0.8432 - val_loss: 0.5417 - val_accuracy: 0.8514\n",
      "Epoch 3/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.8564\n",
      "Epoch 00003: val_accuracy improved from 0.85139 to 0.85271, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 17s 78ms/step - loss: 0.5065 - accuracy: 0.8564 - val_loss: 0.5187 - val_accuracy: 0.8527\n",
      "Epoch 4/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8599\n",
      "Epoch 00004: val_accuracy improved from 0.85271 to 0.85399, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 17s 81ms/step - loss: 0.4810 - accuracy: 0.8599 - val_loss: 0.5108 - val_accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.8611\n",
      "Epoch 00005: val_accuracy improved from 0.85399 to 0.85448, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.4688 - accuracy: 0.8611 - val_loss: 0.5062 - val_accuracy: 0.8545\n",
      "Epoch 6/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8612\n",
      "Epoch 00006: val_accuracy improved from 0.85448 to 0.85467, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.4621 - accuracy: 0.8612 - val_loss: 0.5039 - val_accuracy: 0.8547\n",
      "Epoch 7/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8617\n",
      "Epoch 00007: val_accuracy improved from 0.85467 to 0.85490, saving model to models/weights_cnn1.hdf5\n",
      "213/213 [==============================] - 17s 79ms/step - loss: 0.4579 - accuracy: 0.8617 - val_loss: 0.5023 - val_accuracy: 0.8549\n",
      "Epoch 8/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8617\n",
      "Epoch 00008: val_accuracy did not improve from 0.85490\n",
      "213/213 [==============================] - 17s 78ms/step - loss: 0.4550 - accuracy: 0.8617 - val_loss: 0.5024 - val_accuracy: 0.8541\n",
      "Epoch 9/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.8618\n",
      "Epoch 00009: val_accuracy did not improve from 0.85490\n",
      "213/213 [==============================] - 18s 82ms/step - loss: 0.4530 - accuracy: 0.8618 - val_loss: 0.5013 - val_accuracy: 0.8544\n",
      "Epoch 10/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4516 - accuracy: 0.8618\n",
      "Epoch 00010: val_accuracy did not improve from 0.85490\n",
      "213/213 [==============================] - 17s 78ms/step - loss: 0.4516 - accuracy: 0.8618 - val_loss: 0.5016 - val_accuracy: 0.8546\n",
      "236/236 [==============================] - 1s 6ms/step - loss: 0.4548 - accuracy: 0.8625\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 0.5033 - accuracy: 0.8536\n",
      "training loss: 0.4547775387763977 training accuracy 0.862478494644165\n",
      "test loss: 0.5033350586891174 test accuracy 0.8535937666893005\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "print(seed)\n",
    "\n",
    "model = build_CNN(input_length=max_sent_length, vocab_size=len(vocab_dict),\n",
    "                  embedding_size=128, hidden_size=128, output_size=len(tag_dict),\n",
    "                  kernel_sizes=[1], num_filters=128, num_mlp_layers=2,\n",
    "                  activation=\"relu\",optimizer=\"Adam\",learning_rate=0.05)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_cnn1.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "print(model.summary())\n",
    "cnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=10, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_cnn1.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 6ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'CHEMICAL' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "CNN. Acc: 0.8400792617663224\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "\n",
    "print(\"CNN. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128, 64)           49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128, 65)           4225      \n",
      "=================================================================\n",
      "Total params: 7,153,665\n",
      "Trainable params: 7,153,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "os.makedirs(\"RNN_models\", exist_ok=True)\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.3318 - accuracy: 0.7488\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78878, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 1.3318 - accuracy: 0.7488 - val_loss: 0.8477 - val_accuracy: 0.7888\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.8189\n",
      "Epoch 00002: val_accuracy improved from 0.78878 to 0.83774, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 92ms/step - loss: 0.7337 - accuracy: 0.8189 - val_loss: 0.6404 - val_accuracy: 0.8377\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.8514\n",
      "Epoch 00003: val_accuracy improved from 0.83774 to 0.85614, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 96ms/step - loss: 0.5789 - accuracy: 0.8514 - val_loss: 0.5479 - val_accuracy: 0.8561\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8657\n",
      "Epoch 00004: val_accuracy improved from 0.85614 to 0.86271, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.4990 - accuracy: 0.8657 - val_loss: 0.5033 - val_accuracy: 0.8627\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.8733\n",
      "Epoch 00005: val_accuracy improved from 0.86271 to 0.86685, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.4538 - accuracy: 0.8733 - val_loss: 0.4792 - val_accuracy: 0.8668\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8788\n",
      "Epoch 00006: val_accuracy improved from 0.86685 to 0.86989, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.4246 - accuracy: 0.8788 - val_loss: 0.4637 - val_accuracy: 0.8699\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8829\n",
      "Epoch 00007: val_accuracy improved from 0.86989 to 0.87153, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.4032 - accuracy: 0.8829 - val_loss: 0.4543 - val_accuracy: 0.8715\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.8863\n",
      "Epoch 00008: val_accuracy improved from 0.87153 to 0.87265, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.3861 - accuracy: 0.8863 - val_loss: 0.4477 - val_accuracy: 0.8726\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8894\n",
      "Epoch 00009: val_accuracy improved from 0.87265 to 0.87411, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.3718 - accuracy: 0.8894 - val_loss: 0.4425 - val_accuracy: 0.8741\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8923\n",
      "Epoch 00010: val_accuracy improved from 0.87411 to 0.87539, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.3599 - accuracy: 0.8923 - val_loss: 0.4395 - val_accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8950\n",
      "Epoch 00011: val_accuracy improved from 0.87539 to 0.87606, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 0.3488 - accuracy: 0.8950 - val_loss: 0.4379 - val_accuracy: 0.8761\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8976\n",
      "Epoch 00012: val_accuracy improved from 0.87606 to 0.87651, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3387 - accuracy: 0.8976 - val_loss: 0.4380 - val_accuracy: 0.8765\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8999\n",
      "Epoch 00013: val_accuracy did not improve from 0.87651\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.3296 - accuracy: 0.8999 - val_loss: 0.4377 - val_accuracy: 0.8765\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9025\n",
      "Epoch 00014: val_accuracy improved from 0.87651 to 0.87731, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.3204 - accuracy: 0.9025 - val_loss: 0.4382 - val_accuracy: 0.8773\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9048\n",
      "Epoch 00015: val_accuracy improved from 0.87731 to 0.87781, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 18s 84ms/step - loss: 0.3123 - accuracy: 0.9048 - val_loss: 0.4381 - val_accuracy: 0.8778\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9069\n",
      "Epoch 00016: val_accuracy did not improve from 0.87781\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 0.3047 - accuracy: 0.9069 - val_loss: 0.4411 - val_accuracy: 0.8772\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9092\n",
      "Epoch 00017: val_accuracy improved from 0.87781 to 0.87819, saving model to models/weights_RNN1.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2972 - accuracy: 0.9092 - val_loss: 0.4436 - val_accuracy: 0.8782\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9112\n",
      "Epoch 00018: val_accuracy did not improve from 0.87819\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2905 - accuracy: 0.9112 - val_loss: 0.4459 - val_accuracy: 0.8777\n",
      "Epoch 00018: early stopping\n",
      "236/236 [==============================] - 2s 9ms/step - loss: 0.3008 - accuracy: 0.9095\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.8749\n",
      "training loss: 0.30079489946365356 training accuracy 0.9095014333724976\n",
      "test loss: 0.45161953568458557 test accuracy 0.8749073147773743\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_RNN1.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_RNN1.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 8ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "RNN:LSTM. Acc: 0.8633602360496399\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"RNN:LSTM. Acc:\", calc_accuracy(train_labels, np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,207,233\n",
      "Trainable params: 7,207,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.2146 - accuracy: 0.7637\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79464, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 1.2146 - accuracy: 0.7637 - val_loss: 0.8219 - val_accuracy: 0.7946\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.7147 - accuracy: 0.8213\n",
      "Epoch 00002: val_accuracy improved from 0.79464 to 0.84164, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.7147 - accuracy: 0.8213 - val_loss: 0.6120 - val_accuracy: 0.8416\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.8584\n",
      "Epoch 00003: val_accuracy improved from 0.84164 to 0.86372, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.5423 - accuracy: 0.8584 - val_loss: 0.5081 - val_accuracy: 0.8637\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.8761\n",
      "Epoch 00004: val_accuracy improved from 0.86372 to 0.87336, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.4520 - accuracy: 0.8761 - val_loss: 0.4567 - val_accuracy: 0.8734\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8870\n",
      "Epoch 00005: val_accuracy improved from 0.87336 to 0.87930, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.3988 - accuracy: 0.8870 - val_loss: 0.4293 - val_accuracy: 0.8793\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.8953\n",
      "Epoch 00006: val_accuracy improved from 0.87930 to 0.88322, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 23s 106ms/step - loss: 0.3628 - accuracy: 0.8953 - val_loss: 0.4117 - val_accuracy: 0.8832\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.9018\n",
      "Epoch 00007: val_accuracy improved from 0.88322 to 0.88687, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.3353 - accuracy: 0.9018 - val_loss: 0.4002 - val_accuracy: 0.8869\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9075\n",
      "Epoch 00008: val_accuracy improved from 0.88687 to 0.88932, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.3130 - accuracy: 0.9075 - val_loss: 0.3935 - val_accuracy: 0.8893\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9124\n",
      "Epoch 00009: val_accuracy improved from 0.88932 to 0.89127, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.2941 - accuracy: 0.9124 - val_loss: 0.3882 - val_accuracy: 0.8913\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.9169\n",
      "Epoch 00010: val_accuracy improved from 0.89127 to 0.89210, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 101ms/step - loss: 0.2778 - accuracy: 0.9169 - val_loss: 0.3858 - val_accuracy: 0.8921\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9210\n",
      "Epoch 00011: val_accuracy improved from 0.89210 to 0.89216, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.2633 - accuracy: 0.9210 - val_loss: 0.3868 - val_accuracy: 0.8922\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9248\n",
      "Epoch 00012: val_accuracy improved from 0.89216 to 0.89350, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2501 - accuracy: 0.9248 - val_loss: 0.3850 - val_accuracy: 0.8935\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9285\n",
      "Epoch 00013: val_accuracy improved from 0.89350 to 0.89446, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.2378 - accuracy: 0.9285 - val_loss: 0.3863 - val_accuracy: 0.8945\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9317\n",
      "Epoch 00014: val_accuracy improved from 0.89446 to 0.89550, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.2267 - accuracy: 0.9317 - val_loss: 0.3916 - val_accuracy: 0.8955\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9349\n",
      "Epoch 00015: val_accuracy did not improve from 0.89550\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.2162 - accuracy: 0.9349 - val_loss: 0.3929 - val_accuracy: 0.8948\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9380\n",
      "Epoch 00016: val_accuracy did not improve from 0.89550\n",
      "213/213 [==============================] - 19s 88ms/step - loss: 0.2061 - accuracy: 0.9380 - val_loss: 0.3975 - val_accuracy: 0.8944\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9410\n",
      "Epoch 00017: val_accuracy improved from 0.89550 to 0.89574, saving model to models/weights_BILSTM1.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.1963 - accuracy: 0.9410 - val_loss: 0.4062 - val_accuracy: 0.8957\n",
      "Epoch 00017: early stopping\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 0.2046 - accuracy: 0.9405\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4170 - accuracy: 0.8927\n",
      "training loss: 0.20457972586154938 training accuracy 0.9404528737068176\n",
      "test loss: 0.4170210659503937 test accuracy 0.8926694989204407\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BILSTM1.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BILSTM1.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 10ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['DATE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-LSTM1. Acc: 0.8827620122074691\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-LSTM1. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BILSTM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 512)          935936    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 16,902,505\n",
      "Trainable params: 16,902,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 256\n",
    "num_rnn_layers = 4\n",
    "num_mlp_layers = 2\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0336 - accuracy: 0.7782\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.81637, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 70s 328ms/step - loss: 1.0336 - accuracy: 0.7782 - val_loss: 0.6885 - val_accuracy: 0.8164\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5729 - accuracy: 0.8467\n",
      "Epoch 00002: val_accuracy improved from 0.81637 to 0.86621, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 67s 314ms/step - loss: 0.5729 - accuracy: 0.8467 - val_loss: 0.4938 - val_accuracy: 0.8662\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4244 - accuracy: 0.8827\n",
      "Epoch 00003: val_accuracy improved from 0.86621 to 0.88309, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 69s 323ms/step - loss: 0.4244 - accuracy: 0.8827 - val_loss: 0.4259 - val_accuracy: 0.8831\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3521 - accuracy: 0.9000\n",
      "Epoch 00004: val_accuracy improved from 0.88309 to 0.89009, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 67s 314ms/step - loss: 0.3521 - accuracy: 0.9000 - val_loss: 0.3961 - val_accuracy: 0.8901\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3055 - accuracy: 0.9114\n",
      "Epoch 00005: val_accuracy improved from 0.89009 to 0.89524, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 68s 321ms/step - loss: 0.3055 - accuracy: 0.9114 - val_loss: 0.3771 - val_accuracy: 0.8952\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9219\n",
      "Epoch 00006: val_accuracy improved from 0.89524 to 0.90042, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 66s 310ms/step - loss: 0.2665 - accuracy: 0.9219 - val_loss: 0.3608 - val_accuracy: 0.9004\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9307\n",
      "Epoch 00007: val_accuracy did not improve from 0.90042\n",
      "213/213 [==============================] - 67s 314ms/step - loss: 0.2347 - accuracy: 0.9307 - val_loss: 0.3599 - val_accuracy: 0.9003\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9388\n",
      "Epoch 00008: val_accuracy improved from 0.90042 to 0.90351, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 80s 375ms/step - loss: 0.2057 - accuracy: 0.9388 - val_loss: 0.3613 - val_accuracy: 0.9035\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9464\n",
      "Epoch 00009: val_accuracy improved from 0.90351 to 0.90527, saving model to models/weights_BILSTM2.hdf5\n",
      "213/213 [==============================] - 68s 320ms/step - loss: 0.1791 - accuracy: 0.9464 - val_loss: 0.3709 - val_accuracy: 0.9053\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9534\n",
      "Epoch 00010: val_accuracy did not improve from 0.90527\n",
      "213/213 [==============================] - 66s 309ms/step - loss: 0.1542 - accuracy: 0.9534 - val_loss: 0.3796 - val_accuracy: 0.9047\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9605\n",
      "Epoch 00011: val_accuracy did not improve from 0.90527\n",
      "213/213 [==============================] - 66s 309ms/step - loss: 0.1309 - accuracy: 0.9605 - val_loss: 0.3998 - val_accuracy: 0.9045\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9668\n",
      "Epoch 00012: val_accuracy did not improve from 0.90527\n",
      "213/213 [==============================] - 66s 312ms/step - loss: 0.1095 - accuracy: 0.9668 - val_loss: 0.4160 - val_accuracy: 0.9027\n",
      "Epoch 00012: early stopping\n",
      "236/236 [==============================] - 17s 73ms/step - loss: 0.1704 - accuracy: 0.9506\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.3772 - accuracy: 0.9034\n",
      "training loss: 0.17038027942180634 training accuracy 0.9506038427352905\n",
      "test loss: 0.3772127330303192 test accuracy 0.9034242630004883\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BILSTM2.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BILSTM2.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 70ms/step\n",
      "[['O' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'GENE_OR_GENOME' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'GROUP' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-LSTM2. Acc: 0.8945124244265092\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-LSTM2. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,182,913\n",
      "Trainable params: 7,182,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.1677 - accuracy: 0.7747\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83097, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 1.1677 - accuracy: 0.7747 - val_loss: 0.6733 - val_accuracy: 0.8310\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.8502\n",
      "Epoch 00002: val_accuracy improved from 0.83097 to 0.86226, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.5732 - accuracy: 0.8502 - val_loss: 0.5104 - val_accuracy: 0.8623\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8741\n",
      "Epoch 00003: val_accuracy improved from 0.86226 to 0.87485, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 91ms/step - loss: 0.4537 - accuracy: 0.8741 - val_loss: 0.4446 - val_accuracy: 0.8749\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8878\n",
      "Epoch 00004: val_accuracy improved from 0.87485 to 0.88204, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.3890 - accuracy: 0.8878 - val_loss: 0.4121 - val_accuracy: 0.8820\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8972\n",
      "Epoch 00005: val_accuracy improved from 0.88204 to 0.88726, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.3481 - accuracy: 0.8972 - val_loss: 0.3934 - val_accuracy: 0.8873\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.9040\n",
      "Epoch 00006: val_accuracy improved from 0.88726 to 0.89061, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.3193 - accuracy: 0.9040 - val_loss: 0.3823 - val_accuracy: 0.8906\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9098\n",
      "Epoch 00007: val_accuracy improved from 0.89061 to 0.89294, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.2969 - accuracy: 0.9098 - val_loss: 0.3754 - val_accuracy: 0.8929\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9145\n",
      "Epoch 00008: val_accuracy improved from 0.89294 to 0.89451, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 20s 95ms/step - loss: 0.2785 - accuracy: 0.9145 - val_loss: 0.3707 - val_accuracy: 0.8945\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9188\n",
      "Epoch 00009: val_accuracy improved from 0.89451 to 0.89636, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2627 - accuracy: 0.9188 - val_loss: 0.3669 - val_accuracy: 0.8964\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9228\n",
      "Epoch 00010: val_accuracy improved from 0.89636 to 0.89766, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.2489 - accuracy: 0.9228 - val_loss: 0.3655 - val_accuracy: 0.8977\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9267\n",
      "Epoch 00011: val_accuracy improved from 0.89766 to 0.89858, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.2362 - accuracy: 0.9267 - val_loss: 0.3668 - val_accuracy: 0.8986\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9302\n",
      "Epoch 00012: val_accuracy improved from 0.89858 to 0.89925, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2241 - accuracy: 0.9302 - val_loss: 0.3663 - val_accuracy: 0.8992\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9337\n",
      "Epoch 00013: val_accuracy improved from 0.89925 to 0.89955, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 21s 96ms/step - loss: 0.2130 - accuracy: 0.9337 - val_loss: 0.3678 - val_accuracy: 0.8996\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9371\n",
      "Epoch 00014: val_accuracy improved from 0.89955 to 0.90113, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.2024 - accuracy: 0.9371 - val_loss: 0.3717 - val_accuracy: 0.9011\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9400\n",
      "Epoch 00015: val_accuracy did not improve from 0.90113\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1926 - accuracy: 0.9400 - val_loss: 0.3726 - val_accuracy: 0.9008\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.2046 - accuracy: 0.9383\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.3845 - accuracy: 0.9000\n",
      "training loss: 0.2046383172273636 training accuracy 0.9382885098457336\n",
      "test loss: 0.3844758868217468 test accuracy 0.9000211954116821\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 10ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['DATE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-GRU. Acc: 0.8907923283867049\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-GRU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU2_RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,257,409\n",
      "Trainable params: 7,257,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.7824\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83859, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 26s 120ms/step - loss: 1.0755 - accuracy: 0.7824 - val_loss: 0.6360 - val_accuracy: 0.8386\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8602\n",
      "Epoch 00002: val_accuracy improved from 0.83859 to 0.87074, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.5329 - accuracy: 0.8602 - val_loss: 0.4760 - val_accuracy: 0.8707\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8847\n",
      "Epoch 00003: val_accuracy improved from 0.87074 to 0.88241, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 112ms/step - loss: 0.4149 - accuracy: 0.8847 - val_loss: 0.4226 - val_accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8971\n",
      "Epoch 00004: val_accuracy improved from 0.88241 to 0.88828, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 114ms/step - loss: 0.3562 - accuracy: 0.8971 - val_loss: 0.3965 - val_accuracy: 0.8883\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.9056\n",
      "Epoch 00005: val_accuracy improved from 0.88828 to 0.89268, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 25s 116ms/step - loss: 0.3179 - accuracy: 0.9056 - val_loss: 0.3818 - val_accuracy: 0.8927\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9128\n",
      "Epoch 00006: val_accuracy improved from 0.89268 to 0.89602, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 0.2894 - accuracy: 0.9128 - val_loss: 0.3703 - val_accuracy: 0.8960\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9190\n",
      "Epoch 00007: val_accuracy improved from 0.89602 to 0.89900, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 25s 119ms/step - loss: 0.2663 - accuracy: 0.9190 - val_loss: 0.3644 - val_accuracy: 0.8990\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9247\n",
      "Epoch 00008: val_accuracy improved from 0.89900 to 0.90045, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 112ms/step - loss: 0.2463 - accuracy: 0.9247 - val_loss: 0.3613 - val_accuracy: 0.9005\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9298\n",
      "Epoch 00009: val_accuracy improved from 0.90045 to 0.90303, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.2289 - accuracy: 0.9298 - val_loss: 0.3606 - val_accuracy: 0.9030\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9346 ETA\n",
      "Epoch 00010: val_accuracy improved from 0.90303 to 0.90428, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 25s 116ms/step - loss: 0.2132 - accuracy: 0.9346 - val_loss: 0.3590 - val_accuracy: 0.9043\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.9389\n",
      "Epoch 00011: val_accuracy improved from 0.90428 to 0.90498, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 114ms/step - loss: 0.1989 - accuracy: 0.9389 - val_loss: 0.3615 - val_accuracy: 0.9050\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9429\n",
      "Epoch 00012: val_accuracy did not improve from 0.90498\n",
      "213/213 [==============================] - 25s 118ms/step - loss: 0.1852 - accuracy: 0.9429 - val_loss: 0.3648 - val_accuracy: 0.9047\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9470\n",
      "Epoch 00013: val_accuracy improved from 0.90498 to 0.90620, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 112ms/step - loss: 0.1726 - accuracy: 0.9470 - val_loss: 0.3690 - val_accuracy: 0.9062\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9504\n",
      "Epoch 00014: val_accuracy improved from 0.90620 to 0.90809, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 0.1614 - accuracy: 0.9504 - val_loss: 0.3750 - val_accuracy: 0.9081\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9539\n",
      "Epoch 00015: val_accuracy did not improve from 0.90809\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 0.1505 - accuracy: 0.9539 - val_loss: 0.3816 - val_accuracy: 0.9062\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 4s 16ms/step - loss: 0.1658 - accuracy: 0.9520\n",
      "30/30 [==============================] - 1s 17ms/step - loss: 0.3821 - accuracy: 0.9049\n",
      "training loss: 0.1657974123954773 training accuracy 0.95197594165802\n",
      "test loss: 0.3820992708206177 test accuracy 0.9049285054206848\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU2_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU2_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 15ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'GENE_OR_GENOME' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-GRU2_RELU. Acc: 0.8961555150568429\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-GRU2_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU3_RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,331,905\n",
      "Trainable params: 7,331,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 3\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.7840\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83728, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 30s 143ms/step - loss: 1.0467 - accuracy: 0.7840 - val_loss: 0.6435 - val_accuracy: 0.8373\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.8608\n",
      "Epoch 00002: val_accuracy improved from 0.83728 to 0.87295, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 27s 129ms/step - loss: 0.5352 - accuracy: 0.8608 - val_loss: 0.4734 - val_accuracy: 0.8730\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8869\n",
      "Epoch 00003: val_accuracy improved from 0.87295 to 0.88429, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 28s 132ms/step - loss: 0.4102 - accuracy: 0.8869 - val_loss: 0.4178 - val_accuracy: 0.8843\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8997\n",
      "Epoch 00004: val_accuracy improved from 0.88429 to 0.89046, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 27s 128ms/step - loss: 0.3495 - accuracy: 0.8997 - val_loss: 0.3914 - val_accuracy: 0.8905\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9087\n",
      "Epoch 00005: val_accuracy improved from 0.89046 to 0.89520, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 28s 130ms/step - loss: 0.3103 - accuracy: 0.9087 - val_loss: 0.3767 - val_accuracy: 0.8952\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9162\n",
      "Epoch 00006: val_accuracy improved from 0.89520 to 0.89796, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 26s 121ms/step - loss: 0.2816 - accuracy: 0.9162 - val_loss: 0.3658 - val_accuracy: 0.8980\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9227\n",
      "Epoch 00007: val_accuracy improved from 0.89796 to 0.90083, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 28s 131ms/step - loss: 0.2577 - accuracy: 0.9227 - val_loss: 0.3601 - val_accuracy: 0.9008\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9285\n",
      "Epoch 00008: val_accuracy improved from 0.90083 to 0.90328, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 26s 124ms/step - loss: 0.2371 - accuracy: 0.9285 - val_loss: 0.3588 - val_accuracy: 0.9033\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9338\n",
      "Epoch 00009: val_accuracy improved from 0.90328 to 0.90461, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 28s 130ms/step - loss: 0.2190 - accuracy: 0.9338 - val_loss: 0.3613 - val_accuracy: 0.9046\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9386\n",
      "Epoch 00010: val_accuracy improved from 0.90461 to 0.90481, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 24s 114ms/step - loss: 0.2030 - accuracy: 0.9386 - val_loss: 0.3579 - val_accuracy: 0.9048\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9430\n",
      "Epoch 00011: val_accuracy improved from 0.90481 to 0.90494, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 27s 125ms/step - loss: 0.1880 - accuracy: 0.9430 - val_loss: 0.3657 - val_accuracy: 0.9049\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9474\n",
      "Epoch 00012: val_accuracy improved from 0.90494 to 0.90570, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 25s 118ms/step - loss: 0.1739 - accuracy: 0.9474 - val_loss: 0.3675 - val_accuracy: 0.9057\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9513\n",
      "Epoch 00013: val_accuracy improved from 0.90570 to 0.90650, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 27s 126ms/step - loss: 0.1613 - accuracy: 0.9513 - val_loss: 0.3723 - val_accuracy: 0.9065\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9547\n",
      "Epoch 00014: val_accuracy improved from 0.90650 to 0.90798, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 26s 124ms/step - loss: 0.1497 - accuracy: 0.9547 - val_loss: 0.3824 - val_accuracy: 0.9080\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9580\n",
      "Epoch 00015: val_accuracy did not improve from 0.90798\n",
      "213/213 [==============================] - 26s 121ms/step - loss: 0.1386 - accuracy: 0.9580 - val_loss: 0.3904 - val_accuracy: 0.9056\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 5s 21ms/step - loss: 0.1567 - accuracy: 0.9551\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.4112 - accuracy: 0.9044\n",
      "training loss: 0.15665793418884277 training accuracy 0.9551317691802979\n",
      "test loss: 0.41121354699134827 test accuracy 0.9044385552406311\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU3_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU3_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 20ms/step\n",
      "[['O' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'CHEMICAL' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-GRU3_RELU. Acc: 0.8956203534959067\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-GRU3_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI-GRU22_RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128, 64)           8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128, 65)           4225      \n",
      "=================================================================\n",
      "Total params: 7,261,505\n",
      "Trainable params: 7,261,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 2\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0529 - accuracy: 0.7793\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84037, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 1.0529 - accuracy: 0.7793 - val_loss: 0.6135 - val_accuracy: 0.8404\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.8627\n",
      "Epoch 00002: val_accuracy improved from 0.84037 to 0.87216, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.5135 - accuracy: 0.8627 - val_loss: 0.4623 - val_accuracy: 0.8722\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.8862\n",
      "Epoch 00003: val_accuracy improved from 0.87216 to 0.88311, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.4019 - accuracy: 0.8862 - val_loss: 0.4149 - val_accuracy: 0.8831\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8987\n",
      "Epoch 00004: val_accuracy improved from 0.88311 to 0.88889, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 21s 101ms/step - loss: 0.3462 - accuracy: 0.8987 - val_loss: 0.3919 - val_accuracy: 0.8889\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9074\n",
      "Epoch 00005: val_accuracy improved from 0.88889 to 0.89375, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.3100 - accuracy: 0.9074 - val_loss: 0.3782 - val_accuracy: 0.8938\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9145\n",
      "Epoch 00006: val_accuracy improved from 0.89375 to 0.89592, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.2829 - accuracy: 0.9145 - val_loss: 0.3686 - val_accuracy: 0.8959\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9206\n",
      "Epoch 00007: val_accuracy improved from 0.89592 to 0.89891, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.2600 - accuracy: 0.9206 - val_loss: 0.3639 - val_accuracy: 0.8989\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9263\n",
      "Epoch 00008: val_accuracy improved from 0.89891 to 0.90195, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 0.2407 - accuracy: 0.9263 - val_loss: 0.3609 - val_accuracy: 0.9019\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.9311\n",
      "Epoch 00009: val_accuracy improved from 0.90195 to 0.90382, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.2234 - accuracy: 0.9311 - val_loss: 0.3642 - val_accuracy: 0.9038\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9359\n",
      "Epoch 00010: val_accuracy improved from 0.90382 to 0.90444, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.2079 - accuracy: 0.9359 - val_loss: 0.3594 - val_accuracy: 0.9044\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9403\n",
      "Epoch 00011: val_accuracy improved from 0.90444 to 0.90485, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.1936 - accuracy: 0.9403 - val_loss: 0.3669 - val_accuracy: 0.9048\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9445\n",
      "Epoch 00012: val_accuracy improved from 0.90485 to 0.90540, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.1799 - accuracy: 0.9445 - val_loss: 0.3679 - val_accuracy: 0.9054\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9483\n",
      "Epoch 00013: val_accuracy improved from 0.90540 to 0.90580, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1678 - accuracy: 0.9483 - val_loss: 0.3733 - val_accuracy: 0.9058\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9518\n",
      "Epoch 00014: val_accuracy improved from 0.90580 to 0.90757, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.1562 - accuracy: 0.9518 - val_loss: 0.3804 - val_accuracy: 0.9076\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9551\n",
      "Epoch 00015: val_accuracy did not improve from 0.90757\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.1459 - accuracy: 0.9551 - val_loss: 0.3869 - val_accuracy: 0.9066\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 4s 15ms/step - loss: 0.1614 - accuracy: 0.9532\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.4037 - accuracy: 0.9027\n",
      "training loss: 0.16138634085655212 training accuracy 0.9531723260879517\n",
      "test loss: 0.40370476245880127 test accuracy 0.9027065634727478\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU22_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU22_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 25ms/step\n",
      "[['O' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'CHEMICAL' 'CHEMICAL' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BI-GRU22_RELU. Acc: 0.8937284850588678\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BI-GRU22_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU22_RELU_emb200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 128, 128)          102144    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 11,278,825\n",
      "Trainable params: 11,278,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0219 - accuracy: 0.7929\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85028, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 41s 194ms/step - loss: 1.0219 - accuracy: 0.7929 - val_loss: 0.5831 - val_accuracy: 0.8503\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8692\n",
      "Epoch 00002: val_accuracy improved from 0.85028 to 0.87741, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.4883 - accuracy: 0.8692 - val_loss: 0.4432 - val_accuracy: 0.8774\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8911\n",
      "Epoch 00003: val_accuracy improved from 0.87741 to 0.88677, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.3830 - accuracy: 0.8911 - val_loss: 0.4001 - val_accuracy: 0.8868\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9029\n",
      "Epoch 00004: val_accuracy improved from 0.88677 to 0.89228, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 180ms/step - loss: 0.3301 - accuracy: 0.9029 - val_loss: 0.3789 - val_accuracy: 0.8923\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.9114\n",
      "Epoch 00005: val_accuracy improved from 0.89228 to 0.89620, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 190ms/step - loss: 0.2950 - accuracy: 0.9114 - val_loss: 0.3667 - val_accuracy: 0.8962\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9184\n",
      "Epoch 00006: val_accuracy improved from 0.89620 to 0.89895, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 185ms/step - loss: 0.2681 - accuracy: 0.9184 - val_loss: 0.3590 - val_accuracy: 0.8989\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9249\n",
      "Epoch 00007: val_accuracy improved from 0.89895 to 0.90244, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 181ms/step - loss: 0.2455 - accuracy: 0.9249 - val_loss: 0.3542 - val_accuracy: 0.9024\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9307\n",
      "Epoch 00008: val_accuracy improved from 0.90244 to 0.90387, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 187ms/step - loss: 0.2254 - accuracy: 0.9307 - val_loss: 0.3537 - val_accuracy: 0.9039\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9360\n",
      "Epoch 00009: val_accuracy improved from 0.90387 to 0.90622, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 186ms/step - loss: 0.2076 - accuracy: 0.9360 - val_loss: 0.3562 - val_accuracy: 0.9062\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9410\n",
      "Epoch 00010: val_accuracy did not improve from 0.90622\n",
      "213/213 [==============================] - 39s 183ms/step - loss: 0.1912 - accuracy: 0.9410 - val_loss: 0.3586 - val_accuracy: 0.9059\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9454\n",
      "Epoch 00011: val_accuracy improved from 0.90622 to 0.90624, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 186ms/step - loss: 0.1765 - accuracy: 0.9454 - val_loss: 0.3643 - val_accuracy: 0.9062\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9499\n",
      "Epoch 00012: val_accuracy did not improve from 0.90624\n",
      "213/213 [==============================] - 39s 185ms/step - loss: 0.1622 - accuracy: 0.9499 - val_loss: 0.3692 - val_accuracy: 0.9058\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9541\n",
      "Epoch 00013: val_accuracy improved from 0.90624 to 0.90749, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 180ms/step - loss: 0.1490 - accuracy: 0.9541 - val_loss: 0.3753 - val_accuracy: 0.9075\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU22_RELU_emb200.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 4s 16ms/step - loss: 0.1536 - accuracy: 0.9559\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.3875 - accuracy: 0.9055\n",
      "training loss: 0.1536434292793274 training accuracy 0.9558627009391785\n",
      "test loss: 0.38750386238098145 test accuracy 0.9054926037788391\n",
      "30/30 [==============================] - 1s 22ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE' 'O' 'O']]\n",
      "BIGRU22_RELU_emb200. Acc: 0.8967687812780236\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU22_RELU_emb200.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BIGRU22_RELU_emb200. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU2_RELU_emb200_h128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 128, 256)          253440    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 128, 256)          296448    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 11,660,393\n",
      "Trainable params: 11,660,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 128 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.9280 - accuracy: 0.8030\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85706, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 43s 201ms/step - loss: 0.9280 - accuracy: 0.8030 - val_loss: 0.5424 - val_accuracy: 0.8571\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.8752\n",
      "Epoch 00002: val_accuracy improved from 0.85706 to 0.88167, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 42s 197ms/step - loss: 0.4534 - accuracy: 0.8752 - val_loss: 0.4199 - val_accuracy: 0.8817\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8957\n",
      "Epoch 00003: val_accuracy improved from 0.88167 to 0.89014, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 51s 238ms/step - loss: 0.3569 - accuracy: 0.8957 - val_loss: 0.3822 - val_accuracy: 0.8901\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9075\n",
      "Epoch 00004: val_accuracy improved from 0.89014 to 0.89565, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 42s 195ms/step - loss: 0.3057 - accuracy: 0.9075 - val_loss: 0.3652 - val_accuracy: 0.8957\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9167\n",
      "Epoch 00005: val_accuracy improved from 0.89565 to 0.89941, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 40s 190ms/step - loss: 0.2706 - accuracy: 0.9167 - val_loss: 0.3544 - val_accuracy: 0.8994\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9243\n",
      "Epoch 00006: val_accuracy improved from 0.89941 to 0.90125, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 42s 195ms/step - loss: 0.2430 - accuracy: 0.9243 - val_loss: 0.3494 - val_accuracy: 0.9013\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9312\n",
      "Epoch 00007: val_accuracy improved from 0.90125 to 0.90398, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 42s 197ms/step - loss: 0.2196 - accuracy: 0.9312 - val_loss: 0.3478 - val_accuracy: 0.9040\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9376\n",
      "Epoch 00008: val_accuracy improved from 0.90398 to 0.90746, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 41s 190ms/step - loss: 0.1986 - accuracy: 0.9376 - val_loss: 0.3498 - val_accuracy: 0.9075\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9435\n",
      "Epoch 00009: val_accuracy improved from 0.90746 to 0.90802, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 42s 198ms/step - loss: 0.1796 - accuracy: 0.9435 - val_loss: 0.3575 - val_accuracy: 0.9080\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9488\n",
      "Epoch 00010: val_accuracy improved from 0.90802 to 0.90883, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 41s 190ms/step - loss: 0.1625 - accuracy: 0.9488 - val_loss: 0.3582 - val_accuracy: 0.9088\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9539\n",
      "Epoch 00011: val_accuracy did not improve from 0.90883\n",
      "213/213 [==============================] - 41s 193ms/step - loss: 0.1462 - accuracy: 0.9539 - val_loss: 0.3672 - val_accuracy: 0.9085\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9590\n",
      "Epoch 00012: val_accuracy did not improve from 0.90883\n",
      "213/213 [==============================] - 40s 190ms/step - loss: 0.1306 - accuracy: 0.9590 - val_loss: 0.3774 - val_accuracy: 0.9077\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 5s 21ms/step - loss: 0.1600 - accuracy: 0.9523\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.3760 - accuracy: 0.9075\n",
      "training loss: 0.15998002886772156 training accuracy 0.9523305296897888\n",
      "test loss: 0.37604665756225586 test accuracy 0.9075158834457397\n",
      "30/30 [==============================] - 1s 20ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'EUKARYOTE' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'ORG' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE' 'O'\n",
      "  'O']]\n",
      "BIGRU2_RELU_emb200_h128. Acc: 0.8989788538864301\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BIGRU2_RELU_emb200_h128. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU3_RELU_emb200_h128_dr20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 128, 256)          253440    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 128, 256)          296448    \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 11,660,393\n",
      "Trainable params: 11,660,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 128 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.2,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.9418 - accuracy: 0.8004\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85604, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 42s 196ms/step - loss: 0.9418 - accuracy: 0.8004 - val_loss: 0.5460 - val_accuracy: 0.8560\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.8713\n",
      "Epoch 00002: val_accuracy improved from 0.85604 to 0.88103, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 193ms/step - loss: 0.4700 - accuracy: 0.8713 - val_loss: 0.4239 - val_accuracy: 0.8810\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.8912\n",
      "Epoch 00003: val_accuracy improved from 0.88103 to 0.88910, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 40s 188ms/step - loss: 0.3750 - accuracy: 0.8912 - val_loss: 0.3863 - val_accuracy: 0.8891\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3256 - accuracy: 0.9024\n",
      "Epoch 00004: val_accuracy improved from 0.88910 to 0.89444, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 39s 185ms/step - loss: 0.3256 - accuracy: 0.9024 - val_loss: 0.3670 - val_accuracy: 0.8944\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9108\n",
      "Epoch 00005: val_accuracy improved from 0.89444 to 0.89786, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 190ms/step - loss: 0.2922 - accuracy: 0.9108 - val_loss: 0.3582 - val_accuracy: 0.8979\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9177\n",
      "Epoch 00006: val_accuracy improved from 0.89786 to 0.90023, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 194ms/step - loss: 0.2664 - accuracy: 0.9177 - val_loss: 0.3508 - val_accuracy: 0.9002\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2447 - accuracy: 0.9237\n",
      "Epoch 00007: val_accuracy improved from 0.90023 to 0.90322, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 39s 185ms/step - loss: 0.2447 - accuracy: 0.9237 - val_loss: 0.3473 - val_accuracy: 0.9032\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9293\n",
      "Epoch 00008: val_accuracy improved from 0.90322 to 0.90340, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 192ms/step - loss: 0.2260 - accuracy: 0.9293 - val_loss: 0.3490 - val_accuracy: 0.9034\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9343\n",
      "Epoch 00009: val_accuracy improved from 0.90340 to 0.90605, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 194ms/step - loss: 0.2092 - accuracy: 0.9343 - val_loss: 0.3501 - val_accuracy: 0.9060\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1938 - accuracy: 0.9390\n",
      "Epoch 00010: val_accuracy improved from 0.90605 to 0.90622, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 40s 186ms/step - loss: 0.1938 - accuracy: 0.9390 - val_loss: 0.3505 - val_accuracy: 0.9062\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9434\n",
      "Epoch 00011: val_accuracy improved from 0.90622 to 0.90652, saving model to models/weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\n",
      "213/213 [==============================] - 41s 191ms/step - loss: 0.1792 - accuracy: 0.9434 - val_loss: 0.3594 - val_accuracy: 0.9065\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9474\n",
      "Epoch 00012: val_accuracy did not improve from 0.90652\n",
      "213/213 [==============================] - 39s 185ms/step - loss: 0.1658 - accuracy: 0.9474 - val_loss: 0.3632 - val_accuracy: 0.9059\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 5s 19ms/step - loss: 0.1650 - accuracy: 0.9501\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.3768 - accuracy: 0.9053\n",
      "training loss: 0.16495263576507568 training accuracy 0.9500830769538879\n",
      "test loss: 0.37676259875297546 test accuracy 0.9053045511245728\n",
      "30/30 [==============================] - 1s 21ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'EUKARYOTE' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE' 'O'\n",
      "  'O']]\n",
      "BIGRU3_RELU_emb200_h128_dr20. Acc: 0.896563394949232\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU3_RELU_emb200_h128_dr20.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BIGRU3_RELU_emb200_h128_dr20. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRU4_RELU_emb200_h256_dr20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 128, 512)          703488    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 128, 512)          1182720   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 128, 512)          1182720   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128, 512)          1182720   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128, 512)          0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128, 256)          131328    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 15,493,481\n",
      "Trainable params: 15,493,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 256\n",
    "num_rnn_layers = 4\n",
    "num_mlp_layers = 2\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.2,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.8366 - accuracy: 0.8100\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.86604, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 67s 315ms/step - loss: 0.8366 - accuracy: 0.8100 - val_loss: 0.4871 - val_accuracy: 0.8660\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.8791\n",
      "Epoch 00002: val_accuracy improved from 0.86604 to 0.88651, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 75s 352ms/step - loss: 0.4270 - accuracy: 0.8791 - val_loss: 0.3973 - val_accuracy: 0.8865\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3432 - accuracy: 0.8985\n",
      "Epoch 00003: val_accuracy improved from 0.88651 to 0.89446, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 76s 355ms/step - loss: 0.3432 - accuracy: 0.8985 - val_loss: 0.3674 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.9105\n",
      "Epoch 00004: val_accuracy improved from 0.89446 to 0.89945, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 75s 351ms/step - loss: 0.2964 - accuracy: 0.9105 - val_loss: 0.3562 - val_accuracy: 0.8995\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9197\n",
      "Epoch 00005: val_accuracy improved from 0.89945 to 0.90395, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 75s 352ms/step - loss: 0.2627 - accuracy: 0.9197 - val_loss: 0.3426 - val_accuracy: 0.9039\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9276\n",
      "Epoch 00006: val_accuracy improved from 0.90395 to 0.90451, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 76s 354ms/step - loss: 0.2351 - accuracy: 0.9276 - val_loss: 0.3417 - val_accuracy: 0.9045\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9342\n",
      "Epoch 00007: val_accuracy improved from 0.90451 to 0.90596, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 76s 355ms/step - loss: 0.2117 - accuracy: 0.9342 - val_loss: 0.3451 - val_accuracy: 0.9060\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9401\n",
      "Epoch 00008: val_accuracy improved from 0.90596 to 0.90920, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 75s 352ms/step - loss: 0.1906 - accuracy: 0.9401 - val_loss: 0.3439 - val_accuracy: 0.9092\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9459\n",
      "Epoch 00009: val_accuracy improved from 0.90920 to 0.91007, saving model to models/weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\n",
      "213/213 [==============================] - 75s 352ms/step - loss: 0.1715 - accuracy: 0.9459 - val_loss: 0.3504 - val_accuracy: 0.9101\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9508\n",
      "Epoch 00010: val_accuracy did not improve from 0.91007\n",
      "213/213 [==============================] - 74s 349ms/step - loss: 0.1548 - accuracy: 0.9508 - val_loss: 0.3556 - val_accuracy: 0.9098\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9555\n",
      "Epoch 00011: val_accuracy did not improve from 0.91007\n",
      "213/213 [==============================] - 75s 354ms/step - loss: 0.1388 - accuracy: 0.9555 - val_loss: 0.3676 - val_accuracy: 0.9095\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 15s 65ms/step - loss: 0.1542 - accuracy: 0.9533\n",
      "30/30 [==============================] - 2s 64ms/step - loss: 0.3581 - accuracy: 0.9087\n",
      "training loss: 0.15418824553489685 training accuracy 0.9533421397209167\n",
      "test loss: 0.35811764001846313 test accuracy 0.9086679220199585\n",
      "30/30 [==============================] - 2s 66ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'GROUP' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "BIGRU4_RELU_emb200_h256_dr20. Acc: 0.9002372067459284\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"BIGRU4_RELU_emb200_h256_dr20. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Based on the validation accuracy, we have chose BIGRU4_RELU_emb200_h256_dr20 as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU4_RELU_emb200_h256_dr20.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "val_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "val_tags_by_idx = np.argmax(val_matrix, axis=2)\n",
    "val_preds = np.array([[idx2tag[p] for p in preds] for preds in val_tags_by_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': test_dict[\"id\"],\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in val_preds]})\n",
    "df.to_csv('val_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[\"IMMUNE_RESPONSE\", \"O\", \"O\", \"GENE_OR_GENOME\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"ORGANISM\", \"ORGANISM\", \"O\", \"O\", \"O\", \"O\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[\"O\", \"O\", \"GROUP\", \"O\", \"DISEASE_OR_SYNDROME\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"VIRUS\", \"O\", \"O\", \"WILDLIFE\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[\"EUKARYOTE\", \"VIRUS\", \"O\", \"O\", \"GENE_OR_GENO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>2945</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"GENE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>2946</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2947</td>\n",
       "      <td>[\"O\", \"DATE\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>2948</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"CELL_COMPONENT\", \"O\", \"O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>2949</td>\n",
       "      <td>[\"CHEMICAL\", \"O\", \"O\", \"O\", \"O\", \"CELL\", \"CELL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             labels\n",
       "0        0  [\"IMMUNE_RESPONSE\", \"O\", \"O\", \"GENE_OR_GENOME\"...\n",
       "1        1  [\"ORGANISM\", \"ORGANISM\", \"O\", \"O\", \"O\", \"O\", \"...\n",
       "2        2  [\"O\", \"O\", \"GROUP\", \"O\", \"DISEASE_OR_SYNDROME\"...\n",
       "3        3  [\"O\", \"O\", \"O\", \"VIRUS\", \"O\", \"O\", \"WILDLIFE\",...\n",
       "4        4  [\"EUKARYOTE\", \"VIRUS\", \"O\", \"O\", \"GENE_OR_GENO...\n",
       "...    ...                                                ...\n",
       "2945  2945  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"GENE...\n",
       "2946  2946  [\"O\", \"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RES...\n",
       "2947  2947  [\"O\", \"DATE\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...\n",
       "2948  2948  [\"O\", \"O\", \"O\", \"O\", \"CELL_COMPONENT\", \"O\", \"O...\n",
       "2949  2949  [\"CHEMICAL\", \"O\", \"O\", \"O\", \"O\", \"CELL\", \"CELL...\n",
       "\n",
       "[2950 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"val_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy:  0.9002372067459284\n"
     ]
    }
   ],
   "source": [
    "print(\"Val accuracy: \", evaluate('val_preds.csv', '/kaggle/input/4901k-project/val.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 62ms/step\n"
     ]
    }
   ],
   "source": [
    "test_matrix = model.predict(test_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "test_tags_by_idx = np.argmax(test_matrix, axis=2)\n",
    "test_preds = np.array([[idx2tag[p] for p in preds] for preds in test_tags_by_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': test_dict[\"id\"],\n",
    "                   'labels': [json.dumps(np.array(preds).tolist()) for preds in test_preds]})\n",
    "df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[\"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RESPONSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"_t_pad_\", \"_t_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"RESEARCH_ACTIVITY\", \"RESEARCH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[\"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"O\", \"O\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>2945</td>\n",
       "      <td>[\"DATE\", \"O\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>2946</td>\n",
       "      <td>[\"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2947</td>\n",
       "      <td>[\"O\", \"O\", \"DISEASE_OR_SYNDROME\", \"CHEMICAL\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2948</th>\n",
       "      <td>2948</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>2949</td>\n",
       "      <td>[\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             labels\n",
       "0        0  [\"O\", \"O\", \"IMMUNE_RESPONSE\", \"IMMUNE_RESPONSE...\n",
       "1        1  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"_t_pad_\", \"_t_...\n",
       "2        2  [\"O\", \"O\", \"O\", \"RESEARCH_ACTIVITY\", \"RESEARCH...\n",
       "3        3  [\"O\", \"O\", \"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"...\n",
       "4        4  [\"O\", \"CHEMICAL\", \"CHEMICAL\", \"O\", \"O\", \"O\", \"...\n",
       "...    ...                                                ...\n",
       "2945  2945  [\"DATE\", \"O\", \"CORONAVIRUS\", \"O\", \"O\", \"O\", \"O...\n",
       "2946  2946  [\"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL\", \"CHEMICAL...\n",
       "2947  2947  [\"O\", \"O\", \"DISEASE_OR_SYNDROME\", \"CHEMICAL\", ...\n",
       "2948  2948  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...\n",
       "2949  2949  [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", ...\n",
       "\n",
       "[2950 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

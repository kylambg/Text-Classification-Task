{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/4901kdataset/val.pkl\n",
      "/kaggle/input/4901kdataset/train.pkl\n",
      "/kaggle/input/4901kdataset/test.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy.random import seed\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, Average,Bidirectional, SimpleRNN, LSTM, GRU, TimeDistributed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "test_dict = pkl.load(open('/kaggle/input/4901kdataset/test.pkl', \"rb\"))\n",
    "train_dict = pkl.load(open('/kaggle/input/4901kdataset/train.pkl', \"rb\"))\n",
    "val_dict = pkl.load(open('/kaggle/input/4901kdataset/val.pkl', \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protection', 'of', 'calves', 'against', 'fatal', 'enteric', 'colibacillosis', 'by', 'orally', 'administered', 'Escherichia', 'coli', 'K99', '-', 'specific', 'monoclonal', 'antibody', '.', 'A', 'monoclonal', 'antibody', '(', 'MCA', ')', 'to', 'enterotoxigenic', 'Escherichia', 'coli', 'K99', 'antigen', 'agglutinated', 'K99+', 'enterotoxigenic', 'E', '.', 'coli', 'strains', 'B44', '(', 'O9', ':', 'K30', ';', 'K99', ';', 'F41', ':', 'H-', ')', 'and', 'B41', '(', 'O101', ':', 'K99', ';', 'F41', ':', 'H-', ')', 'grown', 'at', '37', 'degrees', 'C', 'but', 'not', 'at', '18', 'degrees', 'C.', 'The', 'MCA', ',', 'which', 'was', 'characterized', 'as', 'immunoglobulin', 'G1', ',', 'reacted', 'specifically', 'with', 'K99', 'antigen', 'in', 'an', 'enzyme-linked', 'immunosorbent', 'assay', 'and', 'precipitated', 'radiolabeled', 'K99', 'antigen', '.', 'A', 'total', 'of', '45', 'colostrum', '-fed', 'and', 'colostrum', '-deprived', 'calves', 'were', 'used', 'in', 'three', 'separate', 'trials', 'to', 'determine', 'whether', 'the', 'orally', 'administered', 'K99-specific', 'MCA', 'would', 'prevent', 'diarrhea', 'caused', 'by', 'strain', 'B44']\n",
      "['O', 'O', 'LIVESTOCK', 'O', 'O', 'DISEASE_OR_SYNDROME', 'DISEASE_OR_SYNDROME', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'GENE_OR_GENOME', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'O', 'LIVESTOCK', 'O', 'O', 'O', 'CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'DISEASE_OR_SYNDROME', 'O', 'O', 'O', 'GENE_OR_GENOME']\n",
      "index: 0\n",
      "('Protection', 'O') ('of', 'O') ('calves', 'LIVESTOCK') ('against', 'O') ('fatal', 'O') ('enteric', 'DISEASE_OR_SYNDROME') ('colibacillosis', 'DISEASE_OR_SYNDROME') ('by', 'O') ('orally', 'GENE_OR_GENOME') ('administered', 'GENE_OR_GENOME') ('Escherichia', 'GENE_OR_GENOME') ('coli', 'GENE_OR_GENOME') ('K99', 'GENE_OR_GENOME') ('-', 'O') ('specific', 'CARDINAL') ('monoclonal', 'CARDINAL') ('antibody', 'CARDINAL') ('.', 'O') ('A', 'O') ('monoclonal', 'CHEMICAL') ('antibody', 'CHEMICAL') ('(', 'O') ('MCA', 'GENE_OR_GENOME') (')', 'O') ('to', 'O') ('enterotoxigenic', 'CHEMICAL') ('Escherichia', 'CHEMICAL') ('coli', 'CHEMICAL') ('K99', 'O') ('antigen', 'O') ('agglutinated', 'O') ('K99+', 'GENE_OR_GENOME') ('enterotoxigenic', 'GENE_OR_GENOME') ('E', 'GENE_OR_GENOME') ('.', 'O') ('coli', 'CHEMICAL') ('strains', 'CHEMICAL') ('B44', 'CHEMICAL') ('(', 'O') ('O9', 'O') (':', 'O') ('K30', 'O') (';', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('and', 'O') ('B41', 'CHEMICAL') ('(', 'O') ('O101', 'PRODUCT') (':', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('grown', 'O') ('at', 'O') ('37', 'QUANTITY') ('degrees', 'QUANTITY') ('C', 'O') ('but', 'O') ('not', 'O') ('at', 'O') ('18', 'QUANTITY') ('degrees', 'QUANTITY') ('C.', 'O') ('The', 'O') ('MCA', 'GENE_OR_GENOME') (',', 'O') ('which', 'O') ('was', 'O') ('characterized', 'O') ('as', 'O') ('immunoglobulin', 'GENE_OR_GENOME') ('G1', 'GENE_OR_GENOME') (',', 'O') ('reacted', 'O') ('specifically', 'O') ('with', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('in', 'O') ('an', 'O') ('enzyme-linked', 'CHEMICAL') ('immunosorbent', 'CHEMICAL') ('assay', 'CHEMICAL') ('and', 'O') ('precipitated', 'O') ('radiolabeled', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('.', 'O') ('A', 'O') ('total', 'O') ('of', 'O') ('45', 'O') ('colostrum', 'CHEMICAL') ('-fed', 'O') ('and', 'O') ('colostrum', 'CHEMICAL') ('-deprived', 'O') ('calves', 'LIVESTOCK') ('were', 'O') ('used', 'O') ('in', 'O') ('three', 'CARDINAL') ('separate', 'O') ('trials', 'O') ('to', 'O') ('determine', 'O') ('whether', 'O') ('the', 'O') ('orally', 'O') ('administered', 'O') ('K99-specific', 'O') ('MCA', 'GENE_OR_GENOME') ('would', 'O') ('prevent', 'O') ('diarrhea', 'DISEASE_OR_SYNDROME') ('caused', 'O') ('by', 'O') ('strain', 'O') ('B44', 'GENE_OR_GENOME')\n"
     ]
    }
   ],
   "source": [
    "# an entry of the dataset\n",
    "print(train_dict[\"word_seq\"][0])\n",
    "print(train_dict[\"tag_seq\"][0])\n",
    "print(\"index:\", train_dict[\"id\"][0])\n",
    "zipped = zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0])\n",
    "print(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of the NER tags: 65\n",
      "all the NER tags: {'ANATOMICAL_STRUCTURE', 'PERSON', 'LABORATORY_OR_TEST_RESULT', 'CELL_COMPONENT', 'MOLECULAR_FUNCTION', 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT', 'ORGAN_OR_TISSUE_FUNCTION', 'MONEY', 'VIRUS', 'DAILY_OR_RECREATIONAL_ACTIVITY', 'SUBSTRATE', 'MACHINE_ACTIVITY', 'PRODUCT', 'ORG', 'SIGN_OR_SYMPTOM', 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS', 'EUKARYOTE', 'TIME', 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY', 'EXPERIMENTAL_MODEL_OF_DISEASE', 'BODY_SUBSTANCE', 'LABORATORY_PROCEDURE', 'CHEMICAL', 'MATERIAL', 'LIVESTOCK', 'LANGUAGE', 'GENE_OR_GENOME', 'INJURY_OR_POISONING', 'DISEASE_OR_SYNDROME', 'WILDLIFE', 'DATE', 'CELL_FUNCTION', 'LOC', 'PERCENT', 'GROUP_ATTRIBUTE', 'DIAGNOSTIC_PROCEDURE', 'ORGANISM', 'ORDINAL', 'EDUCATIONAL_ACTIVITY', 'RESEARCH_ACTIVITY', 'CELL_OR_MOLECULAR_DYSFUNCTION', 'NORP', 'INDIVIDUAL_BEHAVIOR', 'EVOLUTION', 'IMMUNE_RESPONSE', 'LAW', 'CARDINAL', 'CELL', 'GPE', 'O', 'TISSUE', 'FOOD', 'VIRAL_PROTEIN', 'PHYSICAL_SCIENCE', 'BACTERIUM', 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE', 'QUANTITY', 'GROUP', '_t_pad_', 'WORK_OF_ART', 'EVENT', 'CORONAVIRUS', 'FAC', 'ARCHAEON', 'SOCIAL_BEHAVIOR'}\n"
     ]
    }
   ],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['word_seq'] = [stem(tokens) for tokens in train_dict[\"word_seq\"]]\n",
    "val_dict[\"word_seq\"] = [stem(tokens) for tokens in val_dict[\"word_seq\"]]\n",
    "test_dict[\"word_seq\"] = [stem(tokens) for tokens in test_dict[\"word_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word vocab: 55469 size of tag_dict: 65\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}            \n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = [[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']]\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags = [[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']]\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (23600, 128) tag size: (23600, 128, 65)\n",
      "validating size: (2950, 128) tag size: (2950, 128, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              kernel_sizes, num_filters, num_mlp_layers,\n",
    "              padding=\"valid\",\n",
    "              strides=1,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param kernel_sizes: the kernel sizes of convolutional layers, type: list\n",
    "    :param num_filters: the number of filters for each kernel, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param padding: the padding method in convolutional layers, type: str\n",
    "    :param strides: the strides in convolutional layers, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a CNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # convolutional layers document: https://keras.io/layers/convolutional\n",
    "    # pooling layers document: https://keras.io/layers/pooling/\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    print(input_length,vocab_size,embedding_size,output_size)\n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ########### Conv-Pool ##########\n",
    "    ################################\n",
    "    # convolutional and pooling layers\n",
    "    cnn_results = list()\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # add convolutional layer\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=(kernel_size,),\n",
    "                      padding=padding,\n",
    "                      strides=strides)(emb)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            conv = BatchNormalization()(conv)\n",
    "        # add activation\n",
    "        conv = Activation(activation)(conv)\n",
    "        # add max-pooling\n",
    "        #maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)\n",
    "        #cnn_results.append(Flatten()(maxpool))\n",
    "        cnn_results.append(conv)\n",
    "    \n",
    "    ################################\n",
    "    ##### Fully Connected Layer ####\n",
    "    ################################\n",
    "    h = Average()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    #h = Concatenate()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    h = Dropout(dropout_rate, seed=0)(h)\n",
    "    #h = Embedding(input_dim=vocab_size,\n",
    "    #                output_dim=embedding_size,\n",
    "    #                input_length=input_length,\n",
    "    #                embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(h)\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add skip connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method seed of numpy.random.mtrand.RandomState object at 0x7f738c1dc9e0>\n",
      "128 55469 128 65\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 128, 128)          16512     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 128)          16512     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,141,441\n",
      "Trainable params: 7,141,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.2096 - accuracy: 0.7647\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.82748, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 1.2096 - accuracy: 0.7647 - val_loss: 0.6638 - val_accuracy: 0.8275\n",
      "Epoch 2/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5815 - accuracy: 0.8432\n",
      "Epoch 00002: val_accuracy improved from 0.82748 to 0.85138, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 16s 75ms/step - loss: 0.5815 - accuracy: 0.8432 - val_loss: 0.5417 - val_accuracy: 0.8514\n",
      "Epoch 3/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5065 - accuracy: 0.8564\n",
      "Epoch 00003: val_accuracy improved from 0.85138 to 0.85267, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.5065 - accuracy: 0.8564 - val_loss: 0.5187 - val_accuracy: 0.8527\n",
      "Epoch 4/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4810 - accuracy: 0.8599\n",
      "Epoch 00004: val_accuracy improved from 0.85267 to 0.85397, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 17s 79ms/step - loss: 0.4810 - accuracy: 0.8599 - val_loss: 0.5108 - val_accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.8611\n",
      "Epoch 00005: val_accuracy improved from 0.85397 to 0.85471, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.4688 - accuracy: 0.8611 - val_loss: 0.5062 - val_accuracy: 0.8547\n",
      "Epoch 6/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8612\n",
      "Epoch 00006: val_accuracy did not improve from 0.85471\n",
      "213/213 [==============================] - 16s 73ms/step - loss: 0.4621 - accuracy: 0.8612 - val_loss: 0.5040 - val_accuracy: 0.8546\n",
      "Epoch 7/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.8617\n",
      "Epoch 00007: val_accuracy improved from 0.85471 to 0.85492, saving model to models/cnn1_weights.hdf5\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.4580 - accuracy: 0.8617 - val_loss: 0.5023 - val_accuracy: 0.8549\n",
      "Epoch 8/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.8617\n",
      "Epoch 00008: val_accuracy did not improve from 0.85492\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.4551 - accuracy: 0.8617 - val_loss: 0.5024 - val_accuracy: 0.8541\n",
      "Epoch 9/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.8618\n",
      "Epoch 00009: val_accuracy did not improve from 0.85492\n",
      "213/213 [==============================] - 16s 75ms/step - loss: 0.4530 - accuracy: 0.8618 - val_loss: 0.5013 - val_accuracy: 0.8545\n",
      "Epoch 10/10\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.8618\n",
      "Epoch 00010: val_accuracy did not improve from 0.85492\n",
      "213/213 [==============================] - 16s 73ms/step - loss: 0.4517 - accuracy: 0.8618 - val_loss: 0.5016 - val_accuracy: 0.8545\n",
      "236/236 [==============================] - 1s 5ms/step - loss: 0.4548 - accuracy: 0.8625\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.5033 - accuracy: 0.8536\n",
      "training loss: 0.4548051357269287 training accuracy 0.8624755144119263\n",
      "test loss: 0.5033063888549805 test accuracy 0.8536123037338257\n"
     ]
    }
   ],
   "source": [
    " os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "print(seed)\n",
    "\n",
    "model = build_CNN(input_length=max_sent_length, vocab_size=len(vocab_dict),\n",
    "                  embedding_size=128, hidden_size=128, output_size=len(tag_dict),\n",
    "                  kernel_sizes=[1], num_filters=128, num_mlp_layers=2,\n",
    "                  activation=\"relu\",optimizer=\"Adam\",learning_rate=0.05)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"CNN_models\", \"cnn1_weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "print(model.summary())\n",
    "cnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=10, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"cnn1_weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function to test accuracy\n",
    "# You could check the validation accuracy to select the best of your models\n",
    "def calc_accuracy(preds, tags, padding_id=\"_t_pad_\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            preds (np.narray): (num_data, length_sentence)\n",
    "            tags  (np.narray): (num_data, length_sentence)\n",
    "        Output:\n",
    "            Proportion of correct prediction. The padding tokens are filtered out.\n",
    "    \"\"\"\n",
    "    preds_flatten = preds.flatten()\n",
    "    tags_flatten = tags.flatten()\n",
    "    non_padding_idx = np.where(tags_flatten!=padding_id)[0]\n",
    "    \n",
    "    return sum(preds_flatten[non_padding_idx]==tags_flatten[non_padding_idx])/len(non_padding_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'CHEMICAL' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']]\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 1, CNN. Acc: 0.8400995111226822\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline 1, CNN. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_RNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\",\n",
    "              return_sequences=True):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param num_rnn_layers: the number of layers of the RNN, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param rnn_type: the type of RNN, type: str\n",
    "    :param bidirectional: whether to use bidirectional rnn, type: bool\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a RNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # recurrent layers document: https://keras.io/layers/recurrent\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    \n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ####### Recurrent Layers #######\n",
    "    ################################\n",
    "    # recurrent layers\n",
    "    # Referennce: https://keras.io/api/layers/#recurrent-layers\n",
    "    if rnn_type == \"rnn\":\n",
    "        fn = SimpleRNN\n",
    "    elif rnn_type == \"lstm\":\n",
    "        fn = LSTM\n",
    "    elif rnn_type == \"gru\":\n",
    "        fn = GRU\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    h = emb\n",
    "    for i in range(num_rnn_layers):\n",
    "        is_last = (i == num_rnn_layers-1)\n",
    "        if bidirectional:\n",
    "            h = Bidirectional(fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True))(h)\n",
    "            # return_sequences:\n",
    "            # Boolean. Whether to return the last output. in the output sequence, or the full sequence.\n",
    "            # [h_1, h_2, ..., h_n] or h_n\n",
    "        else:\n",
    "            h = fn(hidden_size,\n",
    "                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n",
    "                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n",
    "                   return_sequences=True)(h)\n",
    "        h = Dropout(dropout_rate, seed=0)(h)\n",
    "\n",
    "    ################################\n",
    "    #### Fully Connected Layers ####\n",
    "    ################################\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "                      bias_initializer=\"zeros\",\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add residual connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\",\n",
    "              kernel_initializer=keras.initializers.he_normal(seed=0),\n",
    "              bias_initializer=\"zeros\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128, 64)           49408     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128, 65)           4225      \n",
      "=================================================================\n",
      "Total params: 7,153,665\n",
      "Trainable params: 7,153,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "os.makedirs(\"RNN_models\", exist_ok=True)\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=False,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.3318 - accuracy: 0.7488\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.78878, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 83ms/step - loss: 1.3318 - accuracy: 0.7488 - val_loss: 0.8477 - val_accuracy: 0.7888\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.7337 - accuracy: 0.8189\n",
      "Epoch 00002: val_accuracy improved from 0.78878 to 0.83774, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.7337 - accuracy: 0.8189 - val_loss: 0.6404 - val_accuracy: 0.8377\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5789 - accuracy: 0.8514\n",
      "Epoch 00003: val_accuracy improved from 0.83774 to 0.85614, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 82ms/step - loss: 0.5789 - accuracy: 0.8514 - val_loss: 0.5479 - val_accuracy: 0.8561\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8657\n",
      "Epoch 00004: val_accuracy improved from 0.85614 to 0.86271, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 80ms/step - loss: 0.4990 - accuracy: 0.8657 - val_loss: 0.5033 - val_accuracy: 0.8627\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4538 - accuracy: 0.8733\n",
      "Epoch 00005: val_accuracy improved from 0.86271 to 0.86685, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 81ms/step - loss: 0.4538 - accuracy: 0.8733 - val_loss: 0.4792 - val_accuracy: 0.8668\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4246 - accuracy: 0.8788\n",
      "Epoch 00006: val_accuracy improved from 0.86685 to 0.86989, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 78ms/step - loss: 0.4246 - accuracy: 0.8788 - val_loss: 0.4637 - val_accuracy: 0.8699\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8829\n",
      "Epoch 00007: val_accuracy improved from 0.86989 to 0.87153, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 82ms/step - loss: 0.4032 - accuracy: 0.8829 - val_loss: 0.4543 - val_accuracy: 0.8715\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.8863\n",
      "Epoch 00008: val_accuracy improved from 0.87153 to 0.87265, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.3861 - accuracy: 0.8863 - val_loss: 0.4477 - val_accuracy: 0.8726\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8894\n",
      "Epoch 00009: val_accuracy improved from 0.87265 to 0.87411, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 82ms/step - loss: 0.3718 - accuracy: 0.8894 - val_loss: 0.4425 - val_accuracy: 0.8741\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8923\n",
      "Epoch 00010: val_accuracy improved from 0.87411 to 0.87539, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 84ms/step - loss: 0.3599 - accuracy: 0.8923 - val_loss: 0.4395 - val_accuracy: 0.8754\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3488 - accuracy: 0.8950\n",
      "Epoch 00011: val_accuracy improved from 0.87539 to 0.87606, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 79ms/step - loss: 0.3488 - accuracy: 0.8950 - val_loss: 0.4379 - val_accuracy: 0.8761\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8976\n",
      "Epoch 00012: val_accuracy improved from 0.87606 to 0.87651, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 82ms/step - loss: 0.3387 - accuracy: 0.8976 - val_loss: 0.4380 - val_accuracy: 0.8765\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.8999\n",
      "Epoch 00013: val_accuracy did not improve from 0.87651\n",
      "213/213 [==============================] - 16s 77ms/step - loss: 0.3296 - accuracy: 0.8999 - val_loss: 0.4377 - val_accuracy: 0.8765\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9025\n",
      "Epoch 00014: val_accuracy improved from 0.87651 to 0.87731, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3204 - accuracy: 0.9025 - val_loss: 0.4382 - val_accuracy: 0.8773\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9048\n",
      "Epoch 00015: val_accuracy improved from 0.87731 to 0.87781, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 79ms/step - loss: 0.3123 - accuracy: 0.9048 - val_loss: 0.4381 - val_accuracy: 0.8778\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9069\n",
      "Epoch 00016: val_accuracy did not improve from 0.87781\n",
      "213/213 [==============================] - 17s 78ms/step - loss: 0.3047 - accuracy: 0.9069 - val_loss: 0.4411 - val_accuracy: 0.8772\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9092\n",
      "Epoch 00017: val_accuracy improved from 0.87781 to 0.87819, saving model to models/weights.hdf5\n",
      "213/213 [==============================] - 17s 82ms/step - loss: 0.2972 - accuracy: 0.9092 - val_loss: 0.4436 - val_accuracy: 0.8782\n",
      "Epoch 18/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9112\n",
      "Epoch 00018: val_accuracy did not improve from 0.87819\n",
      "213/213 [==============================] - 17s 80ms/step - loss: 0.2905 - accuracy: 0.9112 - val_loss: 0.4459 - val_accuracy: 0.8777\n",
      "Epoch 00018: early stopping\n",
      "236/236 [==============================] - 2s 7ms/step - loss: 0.3008 - accuracy: 0.9095\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.4516 - accuracy: 0.8749\n",
      "training loss: 0.30079489946365356 training accuracy 0.9095014333724976\n",
      "test loss: 0.45161953568458557 test accuracy 0.8749073147773743\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 8ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']]\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 1, RNN:LSTM. Acc: 0.8633602360496399\n"
     ]
    }
   ],
   "source": [
    "print(\"baseline 1, RNN:LSTM. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,207,233\n",
      "Trainable params: 7,207,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"lstm\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.2146 - accuracy: 0.7637\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79464, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 20s 93ms/step - loss: 1.2146 - accuracy: 0.7637 - val_loss: 0.8219 - val_accuracy: 0.7946\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.7147 - accuracy: 0.8213\n",
      "Epoch 00002: val_accuracy improved from 0.79464 to 0.84164, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 0.7147 - accuracy: 0.8213 - val_loss: 0.6120 - val_accuracy: 0.8416\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.8584\n",
      "Epoch 00003: val_accuracy improved from 0.84164 to 0.86372, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 0.5423 - accuracy: 0.8584 - val_loss: 0.5081 - val_accuracy: 0.8637\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.8761\n",
      "Epoch 00004: val_accuracy improved from 0.86372 to 0.87336, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 88ms/step - loss: 0.4520 - accuracy: 0.8761 - val_loss: 0.4567 - val_accuracy: 0.8734\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8870\n",
      "Epoch 00005: val_accuracy improved from 0.87336 to 0.87930, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3988 - accuracy: 0.8870 - val_loss: 0.4293 - val_accuracy: 0.8793\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.8953\n",
      "Epoch 00006: val_accuracy improved from 0.87930 to 0.88322, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 91ms/step - loss: 0.3628 - accuracy: 0.8953 - val_loss: 0.4117 - val_accuracy: 0.8832\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3353 - accuracy: 0.9018\n",
      "Epoch 00007: val_accuracy improved from 0.88322 to 0.88687, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3353 - accuracy: 0.9018 - val_loss: 0.4002 - val_accuracy: 0.8869\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9075\n",
      "Epoch 00008: val_accuracy improved from 0.88687 to 0.88932, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.3130 - accuracy: 0.9075 - val_loss: 0.3935 - val_accuracy: 0.8893\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9124\n",
      "Epoch 00009: val_accuracy improved from 0.88932 to 0.89127, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 91ms/step - loss: 0.2941 - accuracy: 0.9124 - val_loss: 0.3882 - val_accuracy: 0.8913\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.9169\n",
      "Epoch 00010: val_accuracy improved from 0.89127 to 0.89210, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 84ms/step - loss: 0.2778 - accuracy: 0.9169 - val_loss: 0.3858 - val_accuracy: 0.8921\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9210\n",
      "Epoch 00011: val_accuracy improved from 0.89210 to 0.89216, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.2633 - accuracy: 0.9210 - val_loss: 0.3868 - val_accuracy: 0.8922\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9248\n",
      "Epoch 00012: val_accuracy improved from 0.89216 to 0.89350, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 87ms/step - loss: 0.2501 - accuracy: 0.9248 - val_loss: 0.3850 - val_accuracy: 0.8935\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9285\n",
      "Epoch 00013: val_accuracy improved from 0.89350 to 0.89446, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 0.2378 - accuracy: 0.9285 - val_loss: 0.3863 - val_accuracy: 0.8945\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9317\n",
      "Epoch 00014: val_accuracy improved from 0.89446 to 0.89550, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.2267 - accuracy: 0.9317 - val_loss: 0.3916 - val_accuracy: 0.8955\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9349\n",
      "Epoch 00015: val_accuracy did not improve from 0.89550\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.2162 - accuracy: 0.9349 - val_loss: 0.3929 - val_accuracy: 0.8948\n",
      "Epoch 16/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9380\n",
      "Epoch 00016: val_accuracy did not improve from 0.89550\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 0.2061 - accuracy: 0.9380 - val_loss: 0.3975 - val_accuracy: 0.8944\n",
      "Epoch 17/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9410\n",
      "Epoch 00017: val_accuracy improved from 0.89550 to 0.89574, saving model to models/weights_BILSTM.hdf5\n",
      "213/213 [==============================] - 18s 83ms/step - loss: 0.1963 - accuracy: 0.9410 - val_loss: 0.4062 - val_accuracy: 0.8957\n",
      "Epoch 00017: early stopping\n",
      "236/236 [==============================] - 2s 10ms/step - loss: 0.2046 - accuracy: 0.9405\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 0.4170 - accuracy: 0.8927\n",
      "training loss: 0.20457972586154938 training accuracy 0.9404528737068176\n",
      "test loss: 0.4170210659503937 test accuracy 0.8926694989204407\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BILSTM.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BILSTM.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 10ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['GENE_OR_GENOME' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['DATE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "baseline 3, RNN:BI-LSTM. Acc: 0.8827620122074691\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 3, RNN:BI-LSTM. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,182,913\n",
      "Trainable params: 7,182,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 1\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"tanh\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.1677 - accuracy: 0.7747\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83097, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 1.1677 - accuracy: 0.7747 - val_loss: 0.6733 - val_accuracy: 0.8310\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.8502\n",
      "Epoch 00002: val_accuracy improved from 0.83097 to 0.86226, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 89ms/step - loss: 0.5732 - accuracy: 0.8502 - val_loss: 0.5104 - val_accuracy: 0.8623\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4537 - accuracy: 0.8741\n",
      "Epoch 00003: val_accuracy improved from 0.86226 to 0.87485, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 20s 94ms/step - loss: 0.4537 - accuracy: 0.8741 - val_loss: 0.4446 - val_accuracy: 0.8749\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3890 - accuracy: 0.8878\n",
      "Epoch 00004: val_accuracy improved from 0.87485 to 0.88204, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3890 - accuracy: 0.8878 - val_loss: 0.4121 - val_accuracy: 0.8820\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8972\n",
      "Epoch 00005: val_accuracy improved from 0.88204 to 0.88726, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.3481 - accuracy: 0.8972 - val_loss: 0.3934 - val_accuracy: 0.8873\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.9040\n",
      "Epoch 00006: val_accuracy improved from 0.88726 to 0.89061, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 89ms/step - loss: 0.3193 - accuracy: 0.9040 - val_loss: 0.3823 - val_accuracy: 0.8906\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.9098\n",
      "Epoch 00007: val_accuracy improved from 0.89061 to 0.89294, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 88ms/step - loss: 0.2969 - accuracy: 0.9098 - val_loss: 0.3754 - val_accuracy: 0.8929\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2785 - accuracy: 0.9145\n",
      "Epoch 00008: val_accuracy improved from 0.89294 to 0.89451, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 83ms/step - loss: 0.2785 - accuracy: 0.9145 - val_loss: 0.3707 - val_accuracy: 0.8945\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9188\n",
      "Epoch 00009: val_accuracy improved from 0.89451 to 0.89636, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 90ms/step - loss: 0.2627 - accuracy: 0.9188 - val_loss: 0.3669 - val_accuracy: 0.8964\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9228\n",
      "Epoch 00010: val_accuracy improved from 0.89636 to 0.89766, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.2489 - accuracy: 0.9228 - val_loss: 0.3655 - val_accuracy: 0.8977\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9267\n",
      "Epoch 00011: val_accuracy improved from 0.89766 to 0.89858, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 83ms/step - loss: 0.2362 - accuracy: 0.9267 - val_loss: 0.3668 - val_accuracy: 0.8986\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9302\n",
      "Epoch 00012: val_accuracy improved from 0.89858 to 0.89925, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 19s 89ms/step - loss: 0.2241 - accuracy: 0.9302 - val_loss: 0.3663 - val_accuracy: 0.8992\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9337\n",
      "Epoch 00013: val_accuracy improved from 0.89925 to 0.89955, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 87ms/step - loss: 0.2130 - accuracy: 0.9337 - val_loss: 0.3678 - val_accuracy: 0.8996\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9371\n",
      "Epoch 00014: val_accuracy improved from 0.89955 to 0.90113, saving model to models/weights_BIGRU.hdf5\n",
      "213/213 [==============================] - 18s 86ms/step - loss: 0.2024 - accuracy: 0.9371 - val_loss: 0.3717 - val_accuracy: 0.9011\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9400\n",
      "Epoch 00015: val_accuracy did not improve from 0.90113\n",
      "213/213 [==============================] - 18s 85ms/step - loss: 0.1926 - accuracy: 0.9400 - val_loss: 0.3726 - val_accuracy: 0.9008\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 3s 11ms/step - loss: 0.2046 - accuracy: 0.9383\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.3845 - accuracy: 0.9000\n",
      "training loss: 0.2046383172273636 training accuracy 0.9382885098457336\n",
      "test loss: 0.3844758868217468 test accuracy 0.9000211954116821\n"
     ]
    }
   ],
   "source": [
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 10ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['DATE' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "baseline 4, RNN:BI-GRU. Acc: 0.8907923283867049\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 4, RNN:BI-GRU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,257,409\n",
      "Trainable params: 7,257,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0755 - accuracy: 0.7824\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83859, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 1.0755 - accuracy: 0.7824 - val_loss: 0.6360 - val_accuracy: 0.8386\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5329 - accuracy: 0.8602\n",
      "Epoch 00002: val_accuracy improved from 0.83859 to 0.87074, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 22s 103ms/step - loss: 0.5329 - accuracy: 0.8602 - val_loss: 0.4760 - val_accuracy: 0.8707\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8847\n",
      "Epoch 00003: val_accuracy improved from 0.87074 to 0.88241, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.4149 - accuracy: 0.8847 - val_loss: 0.4226 - val_accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8971\n",
      "Epoch 00004: val_accuracy improved from 0.88241 to 0.88828, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.3562 - accuracy: 0.8971 - val_loss: 0.3965 - val_accuracy: 0.8883\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3179 - accuracy: 0.9056\n",
      "Epoch 00005: val_accuracy improved from 0.88828 to 0.89268, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.3179 - accuracy: 0.9056 - val_loss: 0.3818 - val_accuracy: 0.8927\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.9128\n",
      "Epoch 00006: val_accuracy improved from 0.89268 to 0.89602, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.2894 - accuracy: 0.9128 - val_loss: 0.3703 - val_accuracy: 0.8960\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9190\n",
      "Epoch 00007: val_accuracy improved from 0.89602 to 0.89900, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2663 - accuracy: 0.9190 - val_loss: 0.3644 - val_accuracy: 0.8990\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9247\n",
      "Epoch 00008: val_accuracy improved from 0.89900 to 0.90045, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.2463 - accuracy: 0.9247 - val_loss: 0.3613 - val_accuracy: 0.9005\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9298\n",
      "Epoch 00009: val_accuracy improved from 0.90045 to 0.90303, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 22s 102ms/step - loss: 0.2289 - accuracy: 0.9298 - val_loss: 0.3606 - val_accuracy: 0.9030\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9346\n",
      "Epoch 00010: val_accuracy improved from 0.90303 to 0.90428, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.2132 - accuracy: 0.9346 - val_loss: 0.3590 - val_accuracy: 0.9043\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1989 - accuracy: 0.9389\n",
      "Epoch 00011: val_accuracy improved from 0.90428 to 0.90498, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1989 - accuracy: 0.9389 - val_loss: 0.3615 - val_accuracy: 0.9050\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9429\n",
      "Epoch 00012: val_accuracy did not improve from 0.90498\n",
      "213/213 [==============================] - 22s 101ms/step - loss: 0.1852 - accuracy: 0.9429 - val_loss: 0.3648 - val_accuracy: 0.9047\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1726 - accuracy: 0.9470\n",
      "Epoch 00013: val_accuracy improved from 0.90498 to 0.90620, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1726 - accuracy: 0.9470 - val_loss: 0.3690 - val_accuracy: 0.9062\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9504\n",
      "Epoch 00014: val_accuracy improved from 0.90620 to 0.90809, saving model to models/weights_BIGRU2_RELU.hdf5\n",
      "213/213 [==============================] - 21s 98ms/step - loss: 0.1614 - accuracy: 0.9504 - val_loss: 0.3750 - val_accuracy: 0.9081\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.9539\n",
      "Epoch 00015: val_accuracy did not improve from 0.90809\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1505 - accuracy: 0.9539 - val_loss: 0.3816 - val_accuracy: 0.9062\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 0.1658 - accuracy: 0.9520\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.3821 - accuracy: 0.9049\n",
      "training loss: 0.1657974123954773 training accuracy 0.95197594165802\n",
      "test loss: 0.3820992708206177 test accuracy 0.9049285054206848\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU2_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU2_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 14ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'GENE_OR_GENOME' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "baseline 5, RNN:BI-GRU2_RELU. Acc: 0.8961555150568429\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 5, RNN:BI-GRU2_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 7,331,905\n",
      "Trainable params: 7,331,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 3\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0467 - accuracy: 0.7840\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.83728, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 26s 122ms/step - loss: 1.0467 - accuracy: 0.7840 - val_loss: 0.6435 - val_accuracy: 0.8373\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.8608\n",
      "Epoch 00002: val_accuracy improved from 0.83728 to 0.87295, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 24s 113ms/step - loss: 0.5352 - accuracy: 0.8608 - val_loss: 0.4734 - val_accuracy: 0.8730\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8869\n",
      "Epoch 00003: val_accuracy improved from 0.87295 to 0.88429, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.4102 - accuracy: 0.8869 - val_loss: 0.4178 - val_accuracy: 0.8843\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8997\n",
      "Epoch 00004: val_accuracy improved from 0.88429 to 0.89046, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.3495 - accuracy: 0.8997 - val_loss: 0.3914 - val_accuracy: 0.8905\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3103 - accuracy: 0.9087\n",
      "Epoch 00005: val_accuracy improved from 0.89046 to 0.89520, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 24s 112ms/step - loss: 0.3103 - accuracy: 0.9087 - val_loss: 0.3767 - val_accuracy: 0.8952\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9162\n",
      "Epoch 00006: val_accuracy improved from 0.89520 to 0.89796, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 25s 116ms/step - loss: 0.2816 - accuracy: 0.9162 - val_loss: 0.3658 - val_accuracy: 0.8980\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9227\n",
      "Epoch 00007: val_accuracy improved from 0.89796 to 0.90083, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 25s 116ms/step - loss: 0.2577 - accuracy: 0.9227 - val_loss: 0.3601 - val_accuracy: 0.9008\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2371 - accuracy: 0.9285\n",
      "Epoch 00008: val_accuracy improved from 0.90083 to 0.90328, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 24s 111ms/step - loss: 0.2371 - accuracy: 0.9285 - val_loss: 0.3588 - val_accuracy: 0.9033\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9338\n",
      "Epoch 00009: val_accuracy improved from 0.90328 to 0.90461, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 23s 108ms/step - loss: 0.2190 - accuracy: 0.9338 - val_loss: 0.3613 - val_accuracy: 0.9046\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9386\n",
      "Epoch 00010: val_accuracy improved from 0.90461 to 0.90481, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 25s 117ms/step - loss: 0.2030 - accuracy: 0.9386 - val_loss: 0.3579 - val_accuracy: 0.9048\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9430\n",
      "Epoch 00011: val_accuracy improved from 0.90481 to 0.90494, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 23s 109ms/step - loss: 0.1880 - accuracy: 0.9430 - val_loss: 0.3657 - val_accuracy: 0.9049\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1739 - accuracy: 0.9474\n",
      "Epoch 00012: val_accuracy improved from 0.90494 to 0.90570, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 28s 130ms/step - loss: 0.1739 - accuracy: 0.9474 - val_loss: 0.3675 - val_accuracy: 0.9057\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9513\n",
      "Epoch 00013: val_accuracy improved from 0.90570 to 0.90650, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 26s 121ms/step - loss: 0.1613 - accuracy: 0.9513 - val_loss: 0.3723 - val_accuracy: 0.9065\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9547\n",
      "Epoch 00014: val_accuracy improved from 0.90650 to 0.90798, saving model to models/weights_BIGRU3_RELU.hdf5\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.1497 - accuracy: 0.9547 - val_loss: 0.3824 - val_accuracy: 0.9080\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9580\n",
      "Epoch 00015: val_accuracy did not improve from 0.90798\n",
      "213/213 [==============================] - 25s 116ms/step - loss: 0.1386 - accuracy: 0.9580 - val_loss: 0.3904 - val_accuracy: 0.9056\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 4s 17ms/step - loss: 0.1567 - accuracy: 0.9551\n",
      "30/30 [==============================] - 1s 18ms/step - loss: 0.4112 - accuracy: 0.9044\n",
      "training loss: 0.15665793418884277 training accuracy 0.9551317691802979\n",
      "test loss: 0.41121354699134827 test accuracy 0.9044385552406311\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU3_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU3_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 17ms/step\n",
      "[['O' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'ORGANISM' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'CHEMICAL' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "baseline 6, RNN:BI-GRU3_RELU. Acc: 0.8956203534959067\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 6, RNN:BI-GRU3_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 128, 128)          7100032   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128, 64)           8256      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128, 65)           4225      \n",
      "=================================================================\n",
      "Total params: 7,261,505\n",
      "Trainable params: 7,261,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 2\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0529 - accuracy: 0.7793\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84037, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 24s 110ms/step - loss: 1.0529 - accuracy: 0.7793 - val_loss: 0.6135 - val_accuracy: 0.8404\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.8627\n",
      "Epoch 00002: val_accuracy improved from 0.84037 to 0.87216, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.5135 - accuracy: 0.8627 - val_loss: 0.4623 - val_accuracy: 0.8722\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.8862 ETA: 0s - loss: 0.4022 - accuracy\n",
      "Epoch 00003: val_accuracy improved from 0.87216 to 0.88311, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.4019 - accuracy: 0.8862 - val_loss: 0.4149 - val_accuracy: 0.8831\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8987\n",
      "Epoch 00004: val_accuracy improved from 0.88311 to 0.88889, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 25s 119ms/step - loss: 0.3462 - accuracy: 0.8987 - val_loss: 0.3919 - val_accuracy: 0.8889\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9074\n",
      "Epoch 00005: val_accuracy improved from 0.88889 to 0.89375, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.3100 - accuracy: 0.9074 - val_loss: 0.3782 - val_accuracy: 0.8938\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9145\n",
      "Epoch 00006: val_accuracy improved from 0.89375 to 0.89592, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.2829 - accuracy: 0.9145 - val_loss: 0.3686 - val_accuracy: 0.8959\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.9206\n",
      "Epoch 00007: val_accuracy improved from 0.89592 to 0.89891, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 105ms/step - loss: 0.2600 - accuracy: 0.9206 - val_loss: 0.3639 - val_accuracy: 0.8989\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9263\n",
      "Epoch 00008: val_accuracy improved from 0.89891 to 0.90195, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.2407 - accuracy: 0.9263 - val_loss: 0.3609 - val_accuracy: 0.9019\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2234 - accuracy: 0.9311\n",
      "Epoch 00009: val_accuracy improved from 0.90195 to 0.90382, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 21s 99ms/step - loss: 0.2234 - accuracy: 0.9311 - val_loss: 0.3642 - val_accuracy: 0.9038\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9359\n",
      "Epoch 00010: val_accuracy improved from 0.90382 to 0.90444, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 26s 123ms/step - loss: 0.2079 - accuracy: 0.9359 - val_loss: 0.3594 - val_accuracy: 0.9044\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9403\n",
      "Epoch 00011: val_accuracy improved from 0.90444 to 0.90485, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 110ms/step - loss: 0.1936 - accuracy: 0.9403 - val_loss: 0.3669 - val_accuracy: 0.9048\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9445\n",
      "Epoch 00012: val_accuracy improved from 0.90485 to 0.90540, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 23s 107ms/step - loss: 0.1799 - accuracy: 0.9445 - val_loss: 0.3679 - val_accuracy: 0.9054\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9483\n",
      "Epoch 00013: val_accuracy improved from 0.90540 to 0.90580, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 21s 97ms/step - loss: 0.1678 - accuracy: 0.9483 - val_loss: 0.3733 - val_accuracy: 0.9058\n",
      "Epoch 14/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9518\n",
      "Epoch 00014: val_accuracy improved from 0.90580 to 0.90757, saving model to models/weights_BIGRU22_RELU.hdf5\n",
      "213/213 [==============================] - 21s 100ms/step - loss: 0.1562 - accuracy: 0.9518 - val_loss: 0.3804 - val_accuracy: 0.9076\n",
      "Epoch 15/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9551\n",
      "Epoch 00015: val_accuracy did not improve from 0.90757\n",
      "213/213 [==============================] - 22s 104ms/step - loss: 0.1459 - accuracy: 0.9551 - val_loss: 0.3869 - val_accuracy: 0.9066\n",
      "Epoch 00015: early stopping\n",
      "236/236 [==============================] - 3s 13ms/step - loss: 0.1614 - accuracy: 0.9532\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 0.4037 - accuracy: 0.9027\n",
      "training loss: 0.16138634085655212 training accuracy 0.9531723260879517\n",
      "test loss: 0.40370476245880127 test accuracy 0.9027065634727478\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU22_RELU.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU22_RELU.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 15ms/step\n",
      "[['O' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'CHEMICAL' 'CHEMICAL' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'O' 'O' 'O']]\n",
      "baseline 6, RNN:BI-GRU22_RELU. Acc: 0.8937284850588678\n"
     ]
    }
   ],
   "source": [
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 6, RNN:BI-GRU22_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 128, 128)          102144    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 128, 128)          74496     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128, 65)           8385      \n",
      "=================================================================\n",
      "Total params: 11,278,825\n",
      "Trainable params: 11,278,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 64 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 1.0219 - accuracy: 0.7929\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85028, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 39s 182ms/step - loss: 1.0219 - accuracy: 0.7929 - val_loss: 0.5831 - val_accuracy: 0.8503\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.8692\n",
      "Epoch 00002: val_accuracy improved from 0.85028 to 0.87741, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.4883 - accuracy: 0.8692 - val_loss: 0.4432 - val_accuracy: 0.8774\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3830 - accuracy: 0.8911\n",
      "Epoch 00003: val_accuracy improved from 0.87741 to 0.88677, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 40s 188ms/step - loss: 0.3830 - accuracy: 0.8911 - val_loss: 0.4001 - val_accuracy: 0.8868\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.9029\n",
      "Epoch 00004: val_accuracy improved from 0.88677 to 0.89228, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.3301 - accuracy: 0.9029 - val_loss: 0.3789 - val_accuracy: 0.8923\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.9114\n",
      "Epoch 00005: val_accuracy improved from 0.89228 to 0.89620, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2950 - accuracy: 0.9114 - val_loss: 0.3667 - val_accuracy: 0.8962\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.9184\n",
      "Epoch 00006: val_accuracy improved from 0.89620 to 0.89895, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2681 - accuracy: 0.9184 - val_loss: 0.3590 - val_accuracy: 0.8989\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9249\n",
      "Epoch 00007: val_accuracy improved from 0.89895 to 0.90244, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2455 - accuracy: 0.9249 - val_loss: 0.3542 - val_accuracy: 0.9024\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9307\n",
      "Epoch 00008: val_accuracy improved from 0.90244 to 0.90387, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 37s 172ms/step - loss: 0.2254 - accuracy: 0.9307 - val_loss: 0.3537 - val_accuracy: 0.9039\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2076 - accuracy: 0.9360\n",
      "Epoch 00009: val_accuracy improved from 0.90387 to 0.90622, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 38s 177ms/step - loss: 0.2076 - accuracy: 0.9360 - val_loss: 0.3562 - val_accuracy: 0.9062\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9410\n",
      "Epoch 00010: val_accuracy did not improve from 0.90622\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1912 - accuracy: 0.9410 - val_loss: 0.3586 - val_accuracy: 0.9059\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9454\n",
      "Epoch 00011: val_accuracy improved from 0.90622 to 0.90624, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 41s 192ms/step - loss: 0.1765 - accuracy: 0.9454 - val_loss: 0.3643 - val_accuracy: 0.9062\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1622 - accuracy: 0.9499\n",
      "Epoch 00012: val_accuracy did not improve from 0.90624\n",
      "213/213 [==============================] - 41s 192ms/step - loss: 0.1622 - accuracy: 0.9499 - val_loss: 0.3692 - val_accuracy: 0.9058\n",
      "Epoch 13/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1490 - accuracy: 0.9541\n",
      "Epoch 00013: val_accuracy improved from 0.90624 to 0.90749, saving model to models/weights_BIGRU22_RELU_emb200.hdf5\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.1490 - accuracy: 0.9541 - val_loss: 0.3753 - val_accuracy: 0.9075\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: models/weights_BIGRU2_RELU_emb200.hdf5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-06de39600fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     callbacks=[checkpointer, earlystopping])\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weights_BIGRU2_RELU_emb200.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m train_score = model.evaluate(train_tokens, train_tags,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: models/weights_BIGRU2_RELU_emb200.hdf5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU22_RELU_emb200.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 4s 16ms/step - loss: 0.1536 - accuracy: 0.9559\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3875 - accuracy: 0.9055\n",
      "training loss: 0.1536434292793274 training accuracy 0.9558627009391785\n",
      "test loss: 0.38750386238098145 test accuracy 0.9054926037788391\n",
      "30/30 [==============================] - 1s 34ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'ORGANISM' 'O']\n",
      " ['ORGANISM' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CHEMICAL' 'O' 'O' ... 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE' 'O' 'O']]\n",
      "baseline 7, RNN:BI-GRU22_RELU. Acc: 0.8967687812780236\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU22_RELU_emb200.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 7, RNN:BI-GRU22_RELU. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_16 (Embedding)     (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 128, 256)          253440    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 128, 256)          296448    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 11,660,393\n",
      "Trainable params: 11,660,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 128 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.9280 - accuracy: 0.8030\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.85706, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.9280 - accuracy: 0.8030 - val_loss: 0.5424 - val_accuracy: 0.8571\n",
      "Epoch 2/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.8752\n",
      "Epoch 00002: val_accuracy improved from 0.85706 to 0.88167, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 38s 179ms/step - loss: 0.4534 - accuracy: 0.8752 - val_loss: 0.4199 - val_accuracy: 0.8817\n",
      "Epoch 3/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8957\n",
      "Epoch 00003: val_accuracy improved from 0.88167 to 0.89014, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 38s 178ms/step - loss: 0.3569 - accuracy: 0.8957 - val_loss: 0.3822 - val_accuracy: 0.8901\n",
      "Epoch 4/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9075\n",
      "Epoch 00004: val_accuracy improved from 0.89014 to 0.89565, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.3057 - accuracy: 0.9075 - val_loss: 0.3652 - val_accuracy: 0.8957\n",
      "Epoch 5/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.9167\n",
      "Epoch 00005: val_accuracy improved from 0.89565 to 0.89941, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.2706 - accuracy: 0.9167 - val_loss: 0.3544 - val_accuracy: 0.8994\n",
      "Epoch 6/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9243\n",
      "Epoch 00006: val_accuracy improved from 0.89941 to 0.90125, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 37s 174ms/step - loss: 0.2430 - accuracy: 0.9243 - val_loss: 0.3494 - val_accuracy: 0.9013\n",
      "Epoch 7/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9312\n",
      "Epoch 00007: val_accuracy improved from 0.90125 to 0.90398, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 37s 173ms/step - loss: 0.2196 - accuracy: 0.9312 - val_loss: 0.3478 - val_accuracy: 0.9040\n",
      "Epoch 8/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9376\n",
      "Epoch 00008: val_accuracy improved from 0.90398 to 0.90746, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1986 - accuracy: 0.9376 - val_loss: 0.3498 - val_accuracy: 0.9075\n",
      "Epoch 9/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9435\n",
      "Epoch 00009: val_accuracy improved from 0.90746 to 0.90802, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 36s 171ms/step - loss: 0.1796 - accuracy: 0.9435 - val_loss: 0.3575 - val_accuracy: 0.9080\n",
      "Epoch 10/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1625 - accuracy: 0.9488\n",
      "Epoch 00010: val_accuracy improved from 0.90802 to 0.90883, saving model to models/weights_BIGRU2_RELU_emb200_h128.hdf5\n",
      "213/213 [==============================] - 37s 175ms/step - loss: 0.1625 - accuracy: 0.9488 - val_loss: 0.3582 - val_accuracy: 0.9088\n",
      "Epoch 11/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9539\n",
      "Epoch 00011: val_accuracy did not improve from 0.90883\n",
      "213/213 [==============================] - 38s 176ms/step - loss: 0.1462 - accuracy: 0.9539 - val_loss: 0.3672 - val_accuracy: 0.9085\n",
      "Epoch 12/20\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9590\n",
      "Epoch 00012: val_accuracy did not improve from 0.90883\n",
      "213/213 [==============================] - 36s 170ms/step - loss: 0.1306 - accuracy: 0.9590 - val_loss: 0.3774 - val_accuracy: 0.9077\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236/236 [==============================] - 5s 20ms/step - loss: 0.1600 - accuracy: 0.9523\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.3760 - accuracy: 0.9075\n",
      "training loss: 0.15998002886772156 training accuracy 0.9523305296897888\n",
      "test loss: 0.37604665756225586 test accuracy 0.9075158834457397\n",
      "30/30 [==============================] - 1s 20ms/step\n",
      "[['IMMUNE_RESPONSE' 'O' 'O' ... 'O' 'EUKARYOTE' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['O' 'ORG' 'O' ... 'O' 'O' 'O']\n",
      " ...\n",
      " ['O' 'DATE' 'CORONAVIRUS' ... 'O' 'O' 'O']\n",
      " ['O' 'O' 'O' ... 'O' 'O' 'O']\n",
      " ['CELL_COMPONENT' 'O' 'O' ... 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE' 'O'\n",
      "  'O']]\n",
      "baseline 8, RNN:BIGRU2_RELU_emb200_h128. Acc: 0.8989788538864301\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 8, RNN:BIGRU2_RELU_emb200_h128. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_17 (Embedding)     (None, 128, 200)          11093800  \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 128, 256)          253440    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128, 256)          296448    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128, 65)           16705     \n",
      "=================================================================\n",
      "Total params: 11,660,393\n",
      "Trainable params: 11,660,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 128 \n",
    "num_rnn_layers = 2\n",
    "num_mlp_layers = 1\n",
    "model = build_RNN(max_sent_length, len(vocab_dict), embedding_size,\n",
    "              hidden_size, len(tag_dict),\n",
    "              num_rnn_layers, num_mlp_layers,\n",
    "              rnn_type=\"gru\",\n",
    "              bidirectional=True,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.2,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"Adam\",\n",
    "              learning_rate=0.001,\n",
    "              metric=\"accuracy\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144/213 [===================>..........] - ETA: 12s - loss: 1.0951 - accuracy: 0.7812"
     ]
    }
   ],
   "source": [
    "#Fine tuning for BI-GRU\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128_dr20.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1)\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "rnn_history = model.fit(train_tokens, train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20, batch_size=100, verbose=1,\n",
    "                    callbacks=[checkpointer, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(\"models\", \"weights_BIGRU2_RELU_emb200_h128_dr20.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])\n",
    "train_matrix = model.predict(val_tokens,batch_size=100, verbose=1, callbacks=[checkpointer])\n",
    "train_tags_by_idx = np.argmax(train_matrix, axis=2)\n",
    "train_labels = np.array([[idx2tag[p] for p in preds] for preds in train_tags_by_idx])\n",
    "print(train_labels)\n",
    "print(\"baseline 8, RNN:BIGRU2_RELU_emb200_h128_dr20. Acc:\", \n",
    "      calc_accuracy(train_labels, \n",
    "                    np.array([val_dict['tag_seq']])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

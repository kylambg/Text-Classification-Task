{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy.random import seed\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n",
    "    Conv1D, MaxPool1D, Flatten, Concatenate, Add, Average\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys in train_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in val_dict: dict_keys(['id', 'word_seq', 'tag_seq'])\n",
      "keys in test_dict: dict_keys(['id', 'word_seq'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = pkl.load(open(\"data/train.pkl\", \"rb\"))\n",
    "val_dict = pkl.load(open(\"data/val.pkl\", \"rb\"))\n",
    "test_dict = pkl.load(open(\"data/test.pkl\", \"rb\"))\n",
    "print(\"keys in train_dict:\", train_dict.keys())\n",
    "print(\"keys in val_dict:\", val_dict.keys())\n",
    "print(\"keys in test_dict:\", test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(feats, feats_dict):\n",
    "    \"\"\"\n",
    "    :param feats: a list of features, type: list\n",
    "    :param feats_dict: a dict from features to indices, type: dict\n",
    "    return a feature vector,\n",
    "    \"\"\"\n",
    "    # initialize the vector as all zeros\n",
    "    vector = np.zeros(len(feats_dict), dtype=np.float)\n",
    "    for f in feats:\n",
    "        # get the feature index, return -1 if the feature is not existed\n",
    "        f_idx = feats_dict.get(f, -1)\n",
    "        if f_idx != -1:\n",
    "            # set the corresponding element as 1\n",
    "            vector[f_idx] = 1\n",
    "    return vector\n",
    "def stem(tokens):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    return a list of stemmed words, type: list\n",
    "    e.g.\n",
    "    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n",
    "    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n",
    "    \"\"\"\n",
    "    ### equivalent code\n",
    "    # results = list()\n",
    "    # for token in tokens:\n",
    "    #     results.append(ps.stem(token))\n",
    "    # return results\n",
    "\n",
    "    return [ps.stem(token) for token in tokens]\n",
    "def n_gram(tokens, n=1):\n",
    "    \"\"\"\n",
    "    :param tokens: a list of tokens, type: list\n",
    "    :param n: the corresponding n-gram, type: int\n",
    "    return a list of n-gram tokens, type: list\n",
    "    e.g.\n",
    "    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.'], 2\n",
    "    Output: ['text mine', 'mine is', 'is to', 'to identifi', 'identifi use', 'use inform', 'inform .']\n",
    "    \"\"\"\n",
    "    if n == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        results = list()\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            # tokens[i:i+n] will return a sublist from i th to i+n th (i+n th is not included)\n",
    "            results.append(\" \".join(tokens[i:i+n]))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN(input_length, vocab_size, embedding_size,\n",
    "              hidden_size, output_size,\n",
    "              kernel_sizes, num_filters, num_mlp_layers,\n",
    "              padding=\"valid\",\n",
    "              strides=1,\n",
    "              activation=\"relu\",\n",
    "              dropout_rate=0.0,\n",
    "              batch_norm=False,\n",
    "              l2_reg=0.0,\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"SGD\",\n",
    "              learning_rate=0.1,\n",
    "              metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    :param input_length: the maximum length of sentences, type: int\n",
    "    :param vocab_size: the vacabulary size, type: int\n",
    "    :param embedding_size: the dimension of word representations, type: int\n",
    "    :param hidden_size: the dimension of the hidden states, type: int\n",
    "    :param output_size: the dimension of the prediction, type: int\n",
    "    :param kernel_sizes: the kernel sizes of convolutional layers, type: list\n",
    "    :param num_filters: the number of filters for each kernel, type: int\n",
    "    :param num_mlp_layers: the number of layers of the MLP, type: int\n",
    "    :param padding: the padding method in convolutional layers, type: str\n",
    "    :param strides: the strides in convolutional layers, type: int\n",
    "    :param activation: the activation type, type: str\n",
    "    :param dropout_rate: the probability of dropout, type: float\n",
    "    :param batch_norm: whether to enable batch normalization, type: bool\n",
    "    :param l2_reg: the weight for the L2 regularizer, type: str\n",
    "    :param loss: the training loss, type: str\n",
    "    :param optimizer: the optimizer, type: str\n",
    "    :param learning_rate: the learning rate for the optimizer, type: float\n",
    "    :param metric: the metric, type: str\n",
    "    return a CNN for text classification,\n",
    "    # activation document: https://keras.io/activations/\n",
    "    # dropout document: https://keras.io/layers/core/#dropout\n",
    "    # embedding document: https://keras.io/layers/embeddings/#embedding\n",
    "    # convolutional layers document: https://keras.io/layers/convolutional\n",
    "    # pooling layers document: https://keras.io/layers/pooling/\n",
    "    # batch normalization document: https://keras.io/layers/normalization/\n",
    "    # losses document: https://keras.io/losses/\n",
    "    # optimizers document: https://keras.io/optimizers/\n",
    "    # metrics document: https://keras.io/metrics/\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_length,))\n",
    "    print(input_length,vocab_size,embedding_size,output_size)\n",
    "    ################################\n",
    "    ###### Word Representation #####\n",
    "    ################################\n",
    "    # word representation layer\n",
    "    emb = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=input_length,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n",
    "    \n",
    "    ################################\n",
    "    ########### Conv-Pool ##########\n",
    "    ################################\n",
    "    # convolutional and pooling layers\n",
    "    cnn_results = list()\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # add convolutional layer\n",
    "        conv = Conv1D(filters=num_filters,\n",
    "                      kernel_size=(kernel_size,),\n",
    "                      padding=padding,\n",
    "                      strides=strides)(emb)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            conv = BatchNormalization()(conv)\n",
    "        # add activation\n",
    "        conv = Activation(activation)(conv)\n",
    "        # add max-pooling\n",
    "        maxpool = MaxPool1D(pool_size=(input_length-kernel_size)//strides+1)(conv)\n",
    "        cnn_results.append(Flatten()(maxpool))\n",
    "    \n",
    "    ################################\n",
    "    ##### Fully Connected Layer ####\n",
    "    ################################\n",
    "    h = Average()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n",
    "    h = Dropout(dropout_rate, seed=0)(h)\n",
    "    h = Embedding(input_dim=vocab_size,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=1,\n",
    "                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(h)\n",
    "    # multi-layer perceptron\n",
    "    for i in range(num_mlp_layers-1):\n",
    "        new_h = Dense(hidden_size,\n",
    "                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n",
    "        # add batch normalization layer\n",
    "        if batch_norm:\n",
    "            new_h = BatchNormalization()(new_h)\n",
    "        # add skip connection\n",
    "        if i == 0:\n",
    "            h = new_h\n",
    "        else:\n",
    "            h = Add()([h, new_h])\n",
    "        # add activation\n",
    "        h = Activation(activation)(h)\n",
    "    y = Dense(output_size,\n",
    "              activation=\"softmax\")(h)\n",
    "    \n",
    "    # set the loss, the optimizer, and the metric\n",
    "    if optimizer == \"SGD\":\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == \"Adam\":\n",
    "        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    model = Model(x, y)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Protection', 'of', 'calves', 'against', 'fatal', 'enteric', 'colibacillosis', 'by', 'orally', 'administered', 'Escherichia', 'coli', 'K99', '-', 'specific', 'monoclonal', 'antibody', '.', 'A', 'monoclonal', 'antibody', '(', 'MCA', ')', 'to', 'enterotoxigenic', 'Escherichia', 'coli', 'K99', 'antigen', 'agglutinated', 'K99+', 'enterotoxigenic', 'E', '.', 'coli', 'strains', 'B44', '(', 'O9', ':', 'K30', ';', 'K99', ';', 'F41', ':', 'H-', ')', 'and', 'B41', '(', 'O101', ':', 'K99', ';', 'F41', ':', 'H-', ')', 'grown', 'at', '37', 'degrees', 'C', 'but', 'not', 'at', '18', 'degrees', 'C.', 'The', 'MCA', ',', 'which', 'was', 'characterized', 'as', 'immunoglobulin', 'G1', ',', 'reacted', 'specifically', 'with', 'K99', 'antigen', 'in', 'an', 'enzyme-linked', 'immunosorbent', 'assay', 'and', 'precipitated', 'radiolabeled', 'K99', 'antigen', '.', 'A', 'total', 'of', '45', 'colostrum', '-fed', 'and', 'colostrum', '-deprived', 'calves', 'were', 'used', 'in', 'three', 'separate', 'trials', 'to', 'determine', 'whether', 'the', 'orally', 'administered', 'K99-specific', 'MCA', 'would', 'prevent', 'diarrhea', 'caused', 'by', 'strain', 'B44']\n",
      "['O', 'O', 'LIVESTOCK', 'O', 'O', 'DISEASE_OR_SYNDROME', 'DISEASE_OR_SYNDROME', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'GENE_OR_GENOME', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'O', 'O', 'QUANTITY', 'QUANTITY', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'GENE_OR_GENOME', 'O', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'CHEMICAL', 'CHEMICAL', 'O', 'O', 'O', 'O', 'O', 'CHEMICAL', 'O', 'O', 'CHEMICAL', 'O', 'LIVESTOCK', 'O', 'O', 'O', 'CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GENE_OR_GENOME', 'O', 'O', 'DISEASE_OR_SYNDROME', 'O', 'O', 'O', 'GENE_OR_GENOME']\n",
      "index: 0\n",
      "('Protection', 'O') ('of', 'O') ('calves', 'LIVESTOCK') ('against', 'O') ('fatal', 'O') ('enteric', 'DISEASE_OR_SYNDROME') ('colibacillosis', 'DISEASE_OR_SYNDROME') ('by', 'O') ('orally', 'GENE_OR_GENOME') ('administered', 'GENE_OR_GENOME') ('Escherichia', 'GENE_OR_GENOME') ('coli', 'GENE_OR_GENOME') ('K99', 'GENE_OR_GENOME') ('-', 'O') ('specific', 'CARDINAL') ('monoclonal', 'CARDINAL') ('antibody', 'CARDINAL') ('.', 'O') ('A', 'O') ('monoclonal', 'CHEMICAL') ('antibody', 'CHEMICAL') ('(', 'O') ('MCA', 'GENE_OR_GENOME') (')', 'O') ('to', 'O') ('enterotoxigenic', 'CHEMICAL') ('Escherichia', 'CHEMICAL') ('coli', 'CHEMICAL') ('K99', 'O') ('antigen', 'O') ('agglutinated', 'O') ('K99+', 'GENE_OR_GENOME') ('enterotoxigenic', 'GENE_OR_GENOME') ('E', 'GENE_OR_GENOME') ('.', 'O') ('coli', 'CHEMICAL') ('strains', 'CHEMICAL') ('B44', 'CHEMICAL') ('(', 'O') ('O9', 'O') (':', 'O') ('K30', 'O') (';', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('and', 'O') ('B41', 'CHEMICAL') ('(', 'O') ('O101', 'PRODUCT') (':', 'O') ('K99', 'O') (';', 'O') ('F41', 'O') (':', 'O') ('H-', 'O') (')', 'O') ('grown', 'O') ('at', 'O') ('37', 'QUANTITY') ('degrees', 'QUANTITY') ('C', 'O') ('but', 'O') ('not', 'O') ('at', 'O') ('18', 'QUANTITY') ('degrees', 'QUANTITY') ('C.', 'O') ('The', 'O') ('MCA', 'GENE_OR_GENOME') (',', 'O') ('which', 'O') ('was', 'O') ('characterized', 'O') ('as', 'O') ('immunoglobulin', 'GENE_OR_GENOME') ('G1', 'GENE_OR_GENOME') (',', 'O') ('reacted', 'O') ('specifically', 'O') ('with', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('in', 'O') ('an', 'O') ('enzyme-linked', 'CHEMICAL') ('immunosorbent', 'CHEMICAL') ('assay', 'CHEMICAL') ('and', 'O') ('precipitated', 'O') ('radiolabeled', 'O') ('K99', 'CHEMICAL') ('antigen', 'CHEMICAL') ('.', 'O') ('A', 'O') ('total', 'O') ('of', 'O') ('45', 'O') ('colostrum', 'CHEMICAL') ('-fed', 'O') ('and', 'O') ('colostrum', 'CHEMICAL') ('-deprived', 'O') ('calves', 'LIVESTOCK') ('were', 'O') ('used', 'O') ('in', 'O') ('three', 'CARDINAL') ('separate', 'O') ('trials', 'O') ('to', 'O') ('determine', 'O') ('whether', 'O') ('the', 'O') ('orally', 'O') ('administered', 'O') ('K99-specific', 'O') ('MCA', 'GENE_OR_GENOME') ('would', 'O') ('prevent', 'O') ('diarrhea', 'DISEASE_OR_SYNDROME') ('caused', 'O') ('by', 'O') ('strain', 'O') ('B44', 'GENE_OR_GENOME')\n"
     ]
    }
   ],
   "source": [
    "# an entry of the dataset\n",
    "print(train_dict[\"word_seq\"][0])\n",
    "print(train_dict[\"tag_seq\"][0])\n",
    "print(\"index:\", train_dict[\"id\"][0])\n",
    "zipped = zip(train_dict[\"word_seq\"][0], train_dict[\"tag_seq\"][0])\n",
    "print(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of the NER tags: 65\n",
      "all the NER tags: {'ORG', 'IMMUNE_RESPONSE', 'CELL_FUNCTION', 'ARCHAEON', 'LABORATORY_PROCEDURE', 'EVENT', 'BODY_SUBSTANCE', 'CELL_COMPONENT', 'MACHINE_ACTIVITY', 'MONEY', 'LABORATORY_OR_TEST_RESULT', 'THERAPEUTIC_OR_PREVENTIVE_PROCEDURE', 'WORK_OF_ART', 'PHYSICAL_SCIENCE', 'PERSON', 'CELL_OR_MOLECULAR_DYSFUNCTION', 'ORGAN_OR_TISSUE_FUNCTION', 'BODY_PART_ORGAN_OR_ORGAN_COMPONENT', 'NORP', 'HUMAN-CAUSED_PHENOMENON_OR_PROCESS', 'LOC', 'CELL', 'ORDINAL', 'EDUCATIONAL_ACTIVITY', 'TIME', 'LAW', 'GOVERNMENTAL_OR_REGULATORY_ACTIVITY', 'MATERIAL', 'DISEASE_OR_SYNDROME', 'CHEMICAL', 'QUANTITY', 'EVOLUTION', 'DAILY_OR_RECREATIONAL_ACTIVITY', 'GENE_OR_GENOME', 'DIAGNOSTIC_PROCEDURE', 'GROUP', 'VIRUS', 'DATE', 'FAC', 'O', 'VIRAL_PROTEIN', 'RESEARCH_ACTIVITY', 'CARDINAL', 'ANATOMICAL_STRUCTURE', 'SUBSTRATE', 'SOCIAL_BEHAVIOR', 'EUKARYOTE', 'TISSUE', 'SIGN_OR_SYMPTOM', 'LANGUAGE', 'INDIVIDUAL_BEHAVIOR', 'INJURY_OR_POISONING', 'ORGANISM', 'CORONAVIRUS', 'WILDLIFE', 'EXPERIMENTAL_MODEL_OF_DISEASE', '_t_pad_', 'GROUP_ATTRIBUTE', 'PRODUCT', 'BACTERIUM', 'GPE', 'FOOD', 'LIVESTOCK', 'PERCENT', 'MOLECULAR_FUNCTION'}\n"
     ]
    }
   ],
   "source": [
    "# all the NER tags:\n",
    "from itertools import chain\n",
    "print(\"count of the NER tags:\", len(set(chain(*train_dict[\"tag_seq\"]))))\n",
    "print(\"all the NER tags:\", set(chain(*train_dict[\"tag_seq\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['word_seq'] = [stem(tokens) for tokens in train_dict[\"word_seq\"]]\n",
    "val_dict[\"word_seq\"] = [stem(tokens) for tokens in val_dict[\"word_seq\"]]\n",
    "test_dict[\"word_seq\"] = [stem(tokens) for tokens in test_dict[\"word_seq\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word vocab: 55469 size of tag_dict: 65\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {'_unk_': 0, '_w_pad_': 1}\n",
    "\n",
    "for doc in train_dict['word_seq']:\n",
    "    for word in doc:\n",
    "        if(word not in vocab_dict):\n",
    "            vocab_dict[word] = len(vocab_dict)\n",
    "\n",
    "tag_dict = {'_t_pad_': 0} # add a padding token\n",
    "\n",
    "for tag_seq in train_dict['tag_seq']:\n",
    "    for tag in tag_seq:\n",
    "        if(tag not in tag_dict):\n",
    "            tag_dict[tag] = len(tag_dict)\n",
    "word2idx = vocab_dict\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "tag2idx = tag_dict\n",
    "idx2tag = {v:k for k,v in tag2idx.items()}            \n",
    "\n",
    "print(\"size of word vocab:\", len(vocab_dict), \"size of tag_dict:\", len(tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a sentence is set to 128\n",
    "max_sent_length = 128\n",
    "\n",
    "\n",
    "\n",
    "train_tokens = np.array([[word2idx[w] for w in doc] for doc in train_dict['word_seq']])\n",
    "val_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in val_dict['word_seq']])\n",
    "test_tokens = np.array([[word2idx.get(w, 0) for w in doc] for doc in test_dict['word_seq']])\n",
    "\n",
    "\n",
    "train_tags = np.array([[tag2idx[t] for t in t_seq] for t_seq in train_dict['tag_seq']])\n",
    "train_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in train_tags])\n",
    "\n",
    "val_tags =  np.array([[tag2idx[t] for t in t_seq] for t_seq in val_dict['tag_seq']])\n",
    "val_tags = np.array([to_categorical(t_seq, num_classes=len(tag_dict)) for t_seq in val_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (23600, 128) tag size: (23600, 128, 65)\n",
      "validating size: (2950, 128) tag size: (2950, 128, 65)\n"
     ]
    }
   ],
   "source": [
    "print(\"training size:\", train_tokens.shape, \"tag size:\", train_tags.shape)\n",
    "print(\"validating size:\", val_tokens.shape, \"tag size:\", val_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 1, 1, 3, 3, 1, 4, 4, 4, 4, 4, 1, 5, 5, 5, 1, 1, 6, 6, 1,\n",
       "       4, 1, 1, 6, 6, 6, 1, 1, 1, 4, 4, 4, 1, 6, 6, 6, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 6, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 8, 1, 1,\n",
       "       1, 1, 8, 8, 1, 1, 4, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 6, 6, 1, 1,\n",
       "       6, 6, 6, 1, 1, 1, 6, 6, 1, 1, 1, 1, 1, 6, 1, 1, 6, 1, 2, 1, 1, 1,\n",
       "       5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 55469 128 65\n",
      "Model: \"functional_39\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_28 (Embedding)        (None, 128, 128)     7100032     input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 128, 128)     16512       embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 127, 128)     32896       embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 126, 128)     49280       embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, 125, 128)     65664       embedding_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 128, 128)     512         conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 127, 128)     512         conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 126, 128)     512         conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 125, 128)     512         conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 128, 128)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 127, 128)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 126, 128)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 125, 128)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_94 (MaxPooling1D) (None, 1, 128)       0           activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling1D) (None, 1, 128)       0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling1D) (None, 1, 128)       0           activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling1D) (None, 1, 128)       0           activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_92 (Flatten)            (None, 128)          0           max_pooling1d_94[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_93 (Flatten)            (None, 128)          0           max_pooling1d_95[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_94 (Flatten)            (None, 128)          0           max_pooling1d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_95 (Flatten)            (None, 128)          0           max_pooling1d_97[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 128)          0           flatten_92[0][0]                 \n",
      "                                                                 flatten_93[0][0]                 \n",
      "                                                                 flatten_94[0][0]                 \n",
      "                                                                 flatten_95[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 128)          0           average[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_29 (Embedding)        (None, 128, 128)     7100032     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 128, 128)     16512       embedding_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 128, 128)     512         dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 128, 128)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 128, 128)     16512       activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 128, 128)     512         dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 128, 128)     0           activation_136[0][0]             \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 128, 128)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 128, 65)      8385        activation_137[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 14,408,897\n",
      "Trainable params: 14,407,361\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_28/embeddings:0', 'conv1d_94/kernel:0', 'conv1d_94/bias:0', 'conv1d_95/kernel:0', 'conv1d_95/bias:0', 'conv1d_96/kernel:0', 'conv1d_96/bias:0', 'conv1d_97/kernel:0', 'conv1d_97/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_28/embeddings:0', 'conv1d_94/kernel:0', 'conv1d_94/bias:0', 'conv1d_95/kernel:0', 'conv1d_95/bias:0', 'conv1d_96/kernel:0', 'conv1d_96/bias:0', 'conv1d_97/kernel:0', 'conv1d_97/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_28/embeddings:0', 'conv1d_94/kernel:0', 'conv1d_94/bias:0', 'conv1d_95/kernel:0', 'conv1d_95/bias:0', 'conv1d_96/kernel:0', 'conv1d_96/bias:0', 'conv1d_97/kernel:0', 'conv1d_97/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding_28/embeddings:0', 'conv1d_94/kernel:0', 'conv1d_94/bias:0', 'conv1d_95/kernel:0', 'conv1d_95/bias:0', 'conv1d_96/kernel:0', 'conv1d_96/bias:0', 'conv1d_97/kernel:0', 'conv1d_97/bias:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'batch_normalization_49/gamma:0', 'batch_normalization_49/beta:0'] when minimizing the loss.\n",
      "236/236 [==============================] - 12s 51ms/step - loss: 1.4690 - accuracy: 0.7280\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 1.4800 - accuracy: 0.7227\n",
      "training loss: 1.4689805507659912 training accuracy 0.7280279994010925\n",
      "test loss: 1.480031132698059 test accuracy 0.7227463126182556\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = build_CNN(input_length=max_sent_length, vocab_size=len(vocab_dict),\n",
    "                  embedding_size=128, hidden_size=128, output_size=len(tag_dict),\n",
    "                  kernel_sizes=[1,2,3,4], num_filters=128, num_mlp_layers=3,\n",
    "                  activation=\"relu\",\n",
    "                  dropout_rate=0.3, l2_reg=0.05, batch_norm=True)\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(\"models\", \"cnn1_weights.hdf5\"),\n",
    "    monitor=\"val_accuracy\",\n",
    "    verbose=0,\n",
    "    save_best_only=True)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "cnn_history = model.fit(x=train_tokens, y=train_tags,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=128, batch_size=128, verbose=0,\n",
    "                    callbacks=[checkpointer])\n",
    "model = keras.models.load_model(os.path.join(\"models\", \"cnn1_weights.hdf5\"))\n",
    "\n",
    "train_score = model.evaluate(train_tokens, train_tags,\n",
    "                             batch_size=100)\n",
    "test_score = model.evaluate(val_tokens, val_tags,\n",
    "                            batch_size=100)\n",
    "print(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\n",
    "print(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
